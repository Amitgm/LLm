{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "c:\\Users\\Amit\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"LoihiPreprint-IEEEMicroJan18.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/322548911\\nLoihi: A Neu romorphic Manycore Processor with On-Chip Learning\\nArticle \\xa0\\xa0 in\\xa0\\xa0IEEE Micr o · Januar y 2018\\nDOI: 10.1109/MM.2018.112130359\\nCITATIONS\\n1,153READS\\n20,604\\n9 author s, including:\\nSome o f the author s of this public ation ar e also w orking on these r elat ed pr ojects:\\nLoihi neur omorphic pr ocessor with on-chip le arning  View pr oject\\nSilic on spin qubits  View pr oject\\nMike Davies\\nIntel\\n16 PUBLICA TIONS \\xa0\\xa0\\xa01,491  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nNarayan Sriniv asa\\nIntel\\n5 PUBLICA TIONS \\xa0\\xa0\\xa01,276  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAndr ew Lines\\nIntel\\n22 PUBLICA TIONS \\xa0\\xa0\\xa02,012  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAndr eas Wild\\nIntel\\n15 PUBLICA TIONS \\xa0\\xa0\\xa01,473  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Mike Davies  on 04 A ugust 2018.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 0}),\n",
       " Document(page_content='1 \\nLoihi: a Neuromorphic Manycore Processor  with \\nOn-Chip Learning  \\nMike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday,  \\nGeorgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, Yuyun Li ao, Chit-Kwan Lin, Andrew Lines,  \\nRuokun Liu, Deepak Mathaikutty, Steve McCoy, Arnab Paul, Jonath an Tse,  \\nGuruguhanathan Venkatar amanan, Yi-Hsin Weng , Andreas Wild, Yoon seok Yang, Hong Wang  \\nContact: mike.davies@intel.com  \\nIntel Labs, Intel Corporation \\nAbstract —Loihi is a 60 mm2 chip fabricated in Intel’s 14nm process that advances the state -of-the-art modeling of spiking neural networks in \\nsilicon. It integrates a wide range of novel features for the f ield, such as hierarchical conne ctivity, dendritic compartments , synaptic delays, and \\nmost importantly programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorit hm, Loihi can solve \\nLASSO optimization problems with over three orders of magnitude  superior energy-delay-product compared to conventional solvers  running on a \\nCPU iso-process/voltage/area. This provides an unambiguous exam ple of spike-based computation outperforming all known conventi onal \\nsolutions. \\nKeywords —Neural nets, Neuromorphic computing, Artificial Intelligence, M achine learning, Computing Methodologies, Other Architecture St yles \\n\\x8b \\n1 I NTRODUCTION \\nEUROSCIENCE offers a bountiful source of inspiration for novel \\nhardware architectures and algorithms. Through their \\ncomplex interactions at large sca les, biological neurons exhibi t an \\nimpressive range of behaviors and properties that we currently struggle to model with modern analytical tools, let alone repli cate \\nwith our design and manufacturing technology. Some of the magic  \\nthat we see in the brain undoubtedly stems from exotic device a nd \\nmaterial properties that will remain out of our fabs’ reach for  \\nmany years to come. Yet highly simplified abstractions of neura l \\nne t w ork s  a re  now  re vol u t i o ni z i n g  computing by solving difficult  \\nand diverse machine learning problems of great practical value.  \\nPerhaps other less simplified models may also yield near-term \\nvalue. \\nArtificial neural networks (ANNs) are reasonably well served \\nb y  t o d a y ’ s  v o n  N e u m a n n  C P U  a r c h i t e c t u r e s  a n d  G P U  v a r i a n t s ,  especially when assisted by coprocessors optimized for streamin g \\nmatrix arithmetic. Spiking neural network models, on the other \\nhand, are exceedingly poorly served by conventional architectures. Just as the value of ANNs was not fully apprecia ted \\nuntil the advent of sufficiently fast CPUs and GPUs, the same c ould \\nbe the case for spiking models—except different computing \\narchitectures will be required. \\nThe neuromorphic computing field of research spans a range \\nof different neuron models and levels of abstraction. Loihi \\n(pronounced ”low-EE-hee”) is one stake in the ground motivated \\nby a particular class of algorithmic results and perspectives f rom \\nour survey of computational neuroscience and recent \\nneuromorphic advances. We approach the field with an eye for \\nm a t h e m a t i c a l  r i g o r ,  t o p - d o w n  m o d e l i n g ,  r a p i d  a r c h i t e c t u r e  \\niteration, and quantitative benchmarking. Our aim is to develop  \\nalgorithms and hardware in a principled way as much as possible . W e  b e g i n  t h i s  p a p e r  w i t h  o u r  d e f i n i t i o n  o f  t h e  S N N  \\ncomputational model and the features that motivated Loihi’s architectural requirements. We then describe the architecture that supports those requirements and provide an overview of the  \\nchip’s asynchronous design implementation. We conclude with \\nsome preliminary 14nm silicon results. \\nImportantly, Section 2.2 presents a result that unambiguously \\ndemonstrates the value of spike-based computation for one \\nfoundational problem. We view this as a significant result in l ight \\nof ongoing debate about the value of spikes as a computational tool in both mainstream and neuromorphic communities. The \\nskepticism towards spikes is well founded, but in our research we \\nhave moved on from this question, given the existence of an example that potentially generalizes to a very broad class of \\nneural networks, namely all recurrent networks. \\n2 S PIKING NEURAL NETWORKS \\nW e  c o n s i d e r  a  s p i k i n g  n e u r a l  n e t w o r k  ( S N N )  a s  a  m o d e l  o f  \\ncomputation with neurons as the basic processing elements. \\nDifferent from artificial neural networks, SNNs incorporate tim e \\nas an explicit dependency in their computations. At some instan t \\nin time, one or more neurons may send out single-bit impulses, \\nthe spike , to neighbors through directed connections known as \\nsynapses , with a potentially nonzero traveling time. Neurons have \\nlocal state variables with rules governing their evolution and \\ntiming of spike generation. Hence the network is a dynamical \\nsystem where individual neurons interact through spikes. N This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 1}),\n",
       " Document(page_content='2 \\n2.1 Spiking Neural Unit \\nA spiking neuron integrates its spike train input in some fashi on, \\nusually by low pass filter, and fires once a state variable exc eeds a \\nthreshold. Mathematically, each spike train is a sum of Dirac d elta \\nfunctions \\u0bde) \\u0bde  where tk i s  t h e  t i m e  o f  t h e  k-th \\nspike. We adopt a variation of the well-known CUBA leaky- integrate-and-fire model that has two internal state variables,  the \\nsynaptic response current \\nui(t) and the membrane potential vi(t).  \\nThe synaptic response current is the sum of filtered input spik e \\ntrains and a constant bias current: \\n \\n\\u0bdc\\n\\u0bddஷ\\u0bdc \\n( 1 ) \\nwhere wij i s  t h e  s y n a p t i c  w e i g h t  f r o m  n e u r o n - j to i, =)ݐ\\n)ݐ(ܪis the synaptic filter impulse response \\nparameterized by the time constant τu with H(t) the unit step \\nfunction, and bi is a constant bias. The synaptic current is further \\nintegrated as the membrane potential, and the neuron sends out \\na spike when its membrane potential passes its firing threshold  θi. \\n \\n−1\\n௩)ݐ \\n2 ) \\nNote that the integration is leaky, as captured by the time \\nconstant τv. vi is initialized with a value less than θi, and is reset to \\n0 right after a spiking event occurs. \\nLoihi, a fully digital architecture, approximates the above \\ncontinuous time dynamics using a fixed-size discrete timestep \\nmodel. In this model, all neurons need to maintain a consistent  \\nunderstanding of time so their distributed dynamics can evolve in \\na well-defined, synchronized manner. It is worth clarifying tha t \\nthese fixed-size, synchronized time steps relate to the algorithmic time of the computation, and need not have a direct relationship \\nto the hardware execution time.   \\n2.2 Computation with Spikes and Fine-grained Parallelism \\nC o m p u t a t i o n s  i n  S N N s  a r e  c a r r i e d  o u t  t h r o u g h  t h e  i n t e r a c t i n g  \\ndynamics of neuron states. An instructive example is the ℓ1-\\nminimizing sparse coding problem, also known as LASSO, which \\nwe can solve with the SNN in Figure 1a using the Spiking Locall y \\nCompetitive Algorithm [2]. The objective of this problem is to determine a sparse set of coefficients that best represents a g iven \\ninput as the linear combination of features from a feature \\ndictionary. The coefficients can be viewed as the activities of  the \\nspiking neurons in Figure 1a that are competing to form an \\naccurate representation of the d ata. By properly configuring th e \\nn e t w o r k ,  i t  c a n  b e  e s t a b l i s h e d  t h a t  a s  t h e  n e t w o r k  d y n a m i c s  evolve, the average spike rates of the neurons will converge to a \\nfixed point, and this fixed point is identical to the solution of the \\noptimization problem. \\nSuch computation exhibits completely different characteristics \\nfrom conventional linear algebra based approaches. Figure 1b compares the computational efficiency of an SNN with the \\nconventional solver FISTA [3] by having them both solve a spars e \\ncoding problem on a single-threaded CPU. The SNN approach (labelled S-LCA) gives a rapid initial drop in error and obtain s a \\ngood approximate solution faster than FISTA. After this, the S- LCA \\nconvergence speed significantly slows down, and FISTA instead finds a much more precise solution quicker. Hence an interestin g \\nefficiency-accuracy tradeoff arises that makes the SNN solution  \\nparticularly attractive for applications that do not require hi ghly \\nprecise solutions, e.g., a solut ion that is 1% within the optim al. \\nThe remarkable algorithmic efficiency of S-LCA can be \\nattributed to its ability to exploit the temporal ordering of s pikes, \\na general property of the SNN co mputational model. In Figure 1a , \\nthe neuron that has the largest external input to win the competition is more likely to spike at the earliest time, causi ng \\nimmediate inhibition of the other neurons. This inhibition \\nhappens with only a single one-to-many spike communication, in contrast to the usual need for all-to-all state exchanges with \\nmatrix arithmetic based solutions such as FISTA and other \\nconventional solvers. This implies that the SNN solution is c o m m u n i c a t i o n  e f f i c i e n t ,  a n d  i t  m a y  s o l v e  t h e  o p t i m i z a t i o n  \\nproblem with a reduced number of arithmetic operations. We \\npoint interested readers to [1] for more discussions. \\nOur CPU-based evaluation has yet to exploit one important \\nadvantage of SNN-based algorithms: the inherent abundant parallelism. The dominant part of SNN computations—the \\nevolution of individual neuron states within a timestep—can all  be \\ncomputed concurrently. However, harnessing such speedup can be a nontrivial task especially on a conventional CPU architect ure. \\nThe parallelizable work for each neuron only consists of a few \\nvariable updates. Given that the  parallel segment of the work c an \\nbe executed very quickly, the underlying architecture must \\nsupport a fine granularity of parallelism with minimal overhead  in \\ncoordinating the order of computations. These observations \\nmotivate fundamental features of the Loihi architecture, \\ndescribed in Section 3. \\n…. \\nInhibitory weights \\nܾ \\n1 \\nܾ \\n2 \\nܾ \\nܰ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n  \\n \\n \\n \\n  \\n \\n  \\n \\n  \\n \\n \\n \\na) (b) \\nFig. 1 : (a) The network topology for solving LASSO. Each neuron \\nreceives the correlation bi between the input data and a predefined \\nfeature vector as its input. Bottom figure shows the evolution of \\nmembrane potential in a 3- neuron example; the spike rates of the \\nneurons stabilizes to fixed values. (b) Algorithmic efficiency \\ncomparison of a solution based on spiking network (S- LCA) and \\nconventional opti mization methods (FISTA). Both algorithms are \\nimplemented on a CPU with single thread. Y- axis is the normalized \\ndifference to the optimal objective function value. Figures tak en from \\n[1] with detailed information therein. This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\n2.3 Learning with Local Information \\nLearning in an SNN refers to adapting the synaptic weights and \\nhence varying the SNN dynamics to a desired one. Similar to \\nconventional machine learning, we wish to express learning as t he \\nminimization of a particular loss function over many training samples. In the sparse coding case, learning involves finding t he \\nset of synaptic weights that allows the best performing sparse \\nrepresentation, expressed as minimizing the sum of all sparse coding losses. Learning in an SNN naturally proceeds in an onli ne \\nmanner, where training samples are sent to the network \\nsequentially. \\nSNN synaptic weight adaptation rules must satisfy a locality \\nconstraint: each weight can only  be accessed and modified by th e \\ndestination neuron, and the rule can only make use of locally \\navailable information, such as the spike trains from the \\npresynaptic (source) and postsynaptic (destination) neurons. Th e \\nlocality constraint imposes a significant challenge on the desi gn of \\nlearning algorithms, as most conventional optimization \\nprocedures do not satisfy it. Although the development of such decentralized learning algorithms is still in active research, some \\npioneering work exists showing the promise of this approach. \\nThey range from the simple Oja’s rule for finding principal components, to the Widrow-Hoff rule for supervised learning and  \\nits generalization to exploit precise spike timing information [4], \\nto the more complex unsupervised sparse dictionary learning using feedback [5] and event-driven random back-propagation \\n[6]. \\nOnce a learning rule satisfies the locality constraint, the \\ninherent parallelism offered by SNNs will then allow the adapti ve \\nnetwork to be scaled up to large sizes in a way that can be computed efficiently. If the rule also minimizes a loss functio n, \\nthen the system will have well defined dynamics. \\nTo support the development of such scalable learning rules, \\nLoihi offers a variety of local information to a programmable \\nsynaptic learning process: \\n• Spike traces corresponding to filtered presynaptic and \\npostsynaptic spike trains with configurable time constants \\n(Section 3.4.4). In particular, a short time constant allows t h e  l e a r n i n g  r u l e  t o  u t i l i z e  p r e c i s e  s p i k e  t i m i n g  \\ni n f o r m a t i o n ,  w h i l e  a  l o n g  t i m e  c o n s t a n t  c a p t u r e s  t h e  \\ninformation in spike rates. \\n• Multiple spike traces for a given spike train filtered with \\ndifferent time constants. This provides support for differential Hebbian learning by measuring perturbations \\nin spike patterns and Bienenstock-Cooper-Munro learning \\nusing triplet STDP [7], among others. \\n• Two additional state variables per synapse, besides the \\nn o r m a l  w e i g h t ,  i n  o r d e r  t o  p r o v i d e  m o r e  f l e x i b i l i t y  f o r  learning. For example, these can be used as synaptic tags \\nfor reinforcement learning. \\n• Reward traces that correspond to special reward spikes \\ncarrying signed impulse values to represent reward or \\npunishment signals for reinforcement learning. Reward \\nspikes are broadcast to defined sets of synapses in the \\nnetwork that may connect to many different source and \\ndestination neurons. Loihi is the first fully integrated digital SNN chip that suppo rts any \\nof the above features. Some small-scale neuromorphic chips with  \\nanalog synapse and neuron circuits have prototyped synaptic \\nplasticity using spike traces, for example [8], but these prior  chips \\nha ve  orde rs  of  m a g ni t ude  l ow e r ne t w ork  c a pa c it y  c om pa re d t o \\nLoihi as well as far less programmability. \\n2.4 Other Computational Primitives \\nLoihi includes several computational primitives related to othe r \\nactive areas of SNN algorithmic research: \\n• Stochastic noise. Uniformly distributed pseudorandom \\nnumbers may be added to a neuron’s synaptic response current, membrane voltage, and refractory delay. This \\nprovides support for algorithms such as Neural Sampling \\n[9], which can solve probabili stic inference and constraint \\nsatisfaction problems using stochastic dynamics and a \\nform of Markov chain Monte Carlo sampling. \\n• Configurable and adaptable synaptic, axon, and refractory \\ndelays. This provides support for novel forms of temporal \\ncomputation such as polychronous dynamics [10], in \\nwhich subsets of neurons may synchronize over periods of \\nvarying timescales. The number of polychronous groups \\nfar exceeds the number of stable attractors in conventional attractor networks, suggesting a productive \\nspace for computational development. \\n• Configurable dendritic tree processing. Neurons in the \\nSNN may be decomposed into a tree of compartment \\nunits, with the neuron’s input synapses distributed over those compartments. Each compartment supports the \\nsame state variables as a neuron, but only the root of the \\ntree (soma compartment) generates spike outputs. The compartments’ state variables are combined in a \\nconfigurable manner by programming different join \\nfunctions for each compartment junction. \\n• Neuron threshold adaptation in support of intrinsic \\nexcitability homeostasis. \\n• Scaling and saturation of synaptic weights in support of \\n“permanence” levels that exceed the range of weights used during inference. \\nThe combination of these features in one device, especially in \\ncombination with Loihi’s learning capabilities, is novel for th e field \\nof SNN silicon implementation. \\n3 A RCHITECTURE \\n3.1 Chip Overview \\nLoihi features a manycore mesh comprising 128 neuromorphic \\ncores, three embedded x86 processor cores, and off-chip communication interfaces that hierarchically extend the mesh in  \\nfour planar directions to other chips. An asynchronous network-\\non-chip (NoC) transports all communication between cores in the  \\nform of packetized messages. The NoC supports write, read \\nrequest, and read response messages for core management and \\nx86-to-x86 messaging, spike messa ges for SNN computation, and \\nbarrier messages for time synchronization between cores. All \\nmessage types may be sourced externally by a host CPU or on-chi p  This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 3}),\n",
       " Document(page_content='4 \\nby the x86 cores, and these may be directed to any on-chip core . \\nMessages may be hierarchically encapsulated for off-chip \\ncommunication over a second-level network. The mesh protocol \\nsupports scaling to 4096 on-chip cores and, via hierarchical addressing, up to 16,384 chips. \\nEach neuromorphic core implements 1,024 primitive spiking \\nneural units (compartments) grouped into sets of trees \\nconstituting neurons.  The compartments, along with their fanin  \\nand fanout connectivity, share configuration and state variable s in \\nten architectural memories. Their state variables are updated i n a \\ntime-multiplexed, pipelined manner every algorithmic timestep. \\nWhen a neuron’s activation exceeds some threshold level, it generates a spike message that is routed to a set of fanout \\ncompartments contained in some number of destination cores. \\nFlexible and well provisioned SNN connectivity features are \\ncrucial for supporting a broad range of workloads. Some desirab le \\nnetworks may call for dense, all-to-all connectivity while othe rs \\nmay call for sparse connectivity; some may have uniform graph \\ndegree distributions, others power law distributions; some may \\nrequire high precision synaptic weights, e.g. to support learning, \\nwhile others can make do with binary connections. As a rule, \\nalgorithmic performance scales with increasing network size, \\nmeasured not only by neuron counts but especially neuron-to-neuron fanout degrees. We see this rule holding all the way to \\nbiological levels (1:10,000). Due to the \\nO(N2) scaling of \\nconnectivity state in the number of fanouts, it becomes an \\nenormous challenge to support networks with high connectivity \\nusing today’s integrated circuit technology. \\nTo address this challenge, Loihi supports a range of features t o \\nrelax the sometimes severe const raints that other neuromorphic \\ndesigns have imposed on the programmer: \\n1) Sparse network compression. Besides a common dense \\nmatrix connectivity model, Loihi supports three sparse \\nm a t r i x  c o m p r e s s i o n  m o d e l s  i n  w h i c h  f a n o u t  n e u r o n  \\nindices are computed based on index state stored with each synapse’s state variables. \\n2) Core-to-core multicast. Any neuron may direct a single \\nspike to any number of destination cores, as the network \\nconnectivity may require. \\n3) Variable synaptic formats. Loihi supports any weight \\nprecision between one and nine bits, signed or unsigned, \\na n d  w e i g h t  p r e c i s i o n s  m a y  b e  m i x e d  ( w i t h  s c a l e  normalization) even within a single neuron’s fanout \\ndistribution. \\n4) Population-based hierarchical connectivity. As a \\ngeneralized weight sharing mechanism, e.g. to support \\nconvolutional neural network types, connectivity \\ntemplates may be defined and mapped to specific population instances during operation. This feature can \\nreduce a network’s required connectivity resources by \\nover an order magnitude. \\nLoihi is the first fully integrated SNN chip that supports any of the \\nabove features. All prior chips , for example the previously mos t \\nsynaptically dense chip [11], store their synapses in dense mat rix \\nform that significantly constrains the space of networks that m ay \\nbe efficiently supported. \\nEach Loihi core includes a programmable learning engine that \\ncan evolve synaptic state variables over time as a function of historical spike activity. In order to support the broadest pos sible \\nclass of rules, the learning engine operates on filtered spike traces. \\nLearning rules are microcode programmable and support a rich selection of input terms and output synaptic target variables. \\nSpecific sets of these rules are associated with a learning profile \\nbound to each synapse to be modified. The profile is mapped by some combination of presynaptic neuron, postsynaptic neuron, or  \\nclass of synapse. The learning engine supports simple pairwise \\nSTDP rules and also much more complicated rules such as triplet  \\nSTDP, reinforcement learning with synaptic tag assignments, and  \\ncomplex rules that reference both rate averaged and spike-timin g \\ntraces. \\nAll logic in the chip is digital, functionally deterministic, a nd \\nimplemented in an asynchronous bundled data design style. This \\nallows spikes to be generated, routed, and consumed in an event -\\ndriven manner with maximal activity gating during idle periods.  \\nThis implementation style is well suited for spiking neural networks that fundamentally feature a high degree of sparseness  \\nin their activity acro ss both space and time. \\n3.2 Mesh Operation \\nFigure 2 shows the operation of the neuromorphic mesh as it \\nexecutes a spiking neural network model. All cores begin at \\nalgorithmic timestep \\nt. Each core independently iterates over its \\nset of neuron compartments, and any neurons that enter a firing \\nstate generate spike messages that the NoC distributes to all c ores     \\n(a) Initial idle state for timestep \\nt. Each square repres ents a core \\nin the mesh containing multiple \\nneurons  (b) Neurons n1 and n2 in cores A and \\nB fire and generate spike messages  (c) Spikes from all other \\nneurons firing on timestep t in \\ncores A and B ar e  distributed  \\nto their destination cores  (d) Each core advanc es its algorithmic \\ntimestep to t+1 as it handshakes \\nwith its neighbors via barrier \\nsynchronization messages  \\nFig. 2: Mesh Operation  \\nThis article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 4}),\n",
       " Document(page_content='5 \\nthat contain their synaptic fanouts. Spike distributions for tw o \\nsuch example neurons n1 and n2 in cores A and B are illustrated in \\nF i g u r e  2 b ,  w i t h  a d d i t i o n a l  s p i k e  d i s t r i b u t i o n s  f r o m  o t h e r  f i r i n g \\nneurons adding to the NoC traffic in Figure 2c. \\nThe NoC distributes spike (and all other) messages according \\nto a dimension-order routing algorithm. The NoC itself only \\nsupports unicast distributions. To multicast spikes, the output  \\nprocess of each core iterates over a list of destination cores for a \\nfiring neuron’s fanout distribution and sends one spike per cor e. \\nFor deadlock protection reasons relating to read and chip-to-ch ip \\nm e s s a g e  t r a n s a c t i o n s ,  t h e  m e s h  u s e s  t w o  i n d e p e n d e n t  p h y s i c a l  r o u t e r  n e t w o r k s .  F o r  b a n d w i d t h  e f f i c i e n c y ,  t h e  c o r e s  a l t e r n a t e  \\nsending their spike messages across the two physical networks. \\nThis is possible because SNN computation does not depend on the  \\nspike sequence ordering within a timestep. \\nAt the end of the timestep, a mechanism is needed to ensure \\nthat all spikes have been delivered and that it’s safe for the cores \\nproceed to timestep \\nt + 1. Rather than using a globally distributed \\nt i m e  r e f e r e n c e  ( c l o c k )  t h a t  m u s t  p e s s i m i z e  f o r  t h e  w o r s t - c a s e  chip-wide network activity, we use a barrier synchronization \\nmechanism, illustrated in Figure 2d. As each core finishes serv icing \\nits compartments for timestep \\nt, it exchanges barrier messages \\nwith its neighboring cores. The barrier messages flush any spik es \\nin flight and, in a second phase, propagate a timestep-advance notification to all cores. As cores receive the second phase of  \\nbarrier messages, they advance their timestep and proceed to \\nupdate compartments for time \\nt + 1. \\nAs long as management activity is restricted to a specific \\n“preemption” phase of the barrier synchronization process that any embedded x86 core or off-chip host may introduce on \\ndemand, the Loihi mesh is provably deadlock free. \\n3.3 Network Connectivity Architecture \\nIn its most abstract formulation, the neural network mapped to \\nthe Loihi architecture is a directed multigraph structure \\n࣡N ,S), \\nwhere N i s  t h e  s e t  o f  n e u r o n s  i n  t h e  n e t w o r k  a n d  S i s  a  s e t  o f  \\nsynapses (edges) connecting pa irs of neurons. Each synapse s ∈ S \\ncorresponds to a 5-tuple: (i,j,wgt,dly,tag), where i,j ∈ N identify \\nthe source and destination neurons of the synapse, and wgt, dly, \\nand tag are integer-valued properties of the synapse. In general, \\nLoihi will autonomously modify the synaptic variables \\n(wgt,dly,tag) according to programmed learning rules. All other \\nnetwork parameters remain constant unless they are modified by x86 core intervention. \\nAn abstract network is mapped to the mesh by assigning \\nneurons to cores, subject to each core’s resource constraints. \\nFigure 3 shows an example of a simple seven neuron network mapped to three cores. Given a particular neuron-to-core \\nmapping for \\nN, each neuron’s synaptic fanin state ( wgt, dly, and \\ntag) must be stored in the core’s synaptic memory. These \\nschematically correspond to the synaptic spike markers in Figur e \\n3. Each neuron’s fanout edges are projected to a list of core-t o-\\ncore edges (colored yellow), and each core-to-core edge is \\nassigned an axon_id identifier unique to each destination core \\n(colored red). The neuron’s synaptic fanout contained within ea ch \\ndestination core is associated with the corresponding axon_id a nd organized as a list of 4-tuples (j,wgt,dly,tag) stored in the synaptic \\nmemory in some suitably compressed form.  When neuron i \\nspikes, the mesh routes each axon_id to the appropriate fanout \\ncore which then expands it to the corresponding synaptic list. \\nThis connectivity architecture can support arbitrary \\nmultigraph networks subject to t he cores’ resource constraints:  \\n1) The total number of neurons assigned to any core may \\nnot exceed 1,024 ( Ncx). \\n2) The total synaptic fanin state mapped to any core must \\nnot exceed 128KB ( Nsyn × 64 b, subject to compression \\nand list alignment considerations.) \\n3) The total number of core-to-core fanout edges mapped \\nto any given core must not exceed 4,096 ( Naxout). This \\ncorresponds to the number of output-side routing slots \\nhighlighted in yellow in Figure 3. \\n4) The total number of distribution lists, associated by \\naxon_id, in any core must not exceed 4,096 ( Naxin). This \\nis the number of input-side axon_id routing slots \\nhighlighted in red in Figure 3. \\nIn practice, constraints 2 and 4 tend to be the most limiting. \\nIn order to exploit structure that may exist in the network ࣡ ,\\nLoihi supports a hierarchical network model. This feature can \\nsignificantly reduce the chip-wide connectivity and synaptic \\nresources needed to map convolutional-style networks in which a  \\ntemplate of synaptic connections is applied to many neurons in a \\nuniform way. \\nFormally, we represent the hierarchical template network as a \\ndirected multigraph ℋ = (࣮,ℰ  )where ࣮ is a set of disjoint neuron \\npopulation types and ℰ defines a set of edges connecting between \\npairs Tsrc,Tdst ∈ ࣮ An edge E ∈ ℰ associated with the (Tsrc,Tdst) \\npopulation type pair is a set of synapses where each s ∈ E \\nconnects a neuron i ∈ Tsrc to to a neuron j ∈ Tdst. \\nIn order to hierarchically compress the resource mapping of \\nthe desired flat network ࣡N ,S), a set of disjoint neuron \\npopulations instances ࣪ must be defined where each P ∈ ࣪ is a \\nsubset of neurons P ⊂ N. Each population instance is associated \\nwith a population type T ∈ ࣮ from the hierarchical template F\\nE\\nDCORE3\\nC\\nB\\nCORE2X\\nA\\nCORE1\\naxon_id 1axon_id 2axon_id 3\\nF\\nE\\nD\\nC\\nB\\nAX\\nNaxin Naxout\\nFig. 3: Neuron-to-neuron mesh routing model  This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 5}),\n",
       " Document(page_content='6 \\nnetwork ℋ. Neurons n ∈ N  belonging to some population \\ninstance P ∈ ࣪ are said to be population-mapped . By configuring \\nthe ℋ connectivity in hardware, the redundant connectivity in ࣡ \\nis implied and doesn’t consume resources, beyond what it takes \\nto map the population-level connectivity of ℋ as if it were a flat \\nnetwork. \\nPopulation-mapped neurons produce population spike \\nmessages whose axon_id fields identify (1) the destination \\npopulation Pdst, (2) the source neuron index i ∈ Psrc within the \\ns o u r c e  p o p u l a t i o n ,  a n d  ( 3 )  t h e  p a r t i c u l a r  e d g e  c o n n e c t i n g  \\nbetween Tsrc and Tdst w h e n  t h e r e  i s  m o r e  t h a n  o n e .  O n e  \\npopulation spike must be sent per destination population rather  \\nthan per destination core, as in the flat case. This marginally  higher \\nlevel of spike traffic is more than offset by the savings in ne twork \\nmapping resources. \\nConvolutional artificial neural networks (ConvNets), in which a  \\nsingle kernel of weights is repeatedly applied to different pat ches \\nof input pixels, is an example class of network that greatly be nefits \\nfrom hierarchy. By treating such a weight kernel as the templat e \\nconnectivity that is applied to the different image patches \\n(population instances), Loihi can support a spiking form of suc h \\nnetworks. The S-LCA network discussed in Section 5.2 features a  \\nsimilar kernel-style convolutional network topology which \\nadditionally includes lateral inhibitory connections between th e \\nfeature neurons of each population instance. \\n3.4 Learning Engine \\n3.4.1 Baseline STDP \\nA number of neuromorphic chip architectures to date have \\nincorporated the most basic form of pairwise, nearest-neighbor \\nspike time dependent plasticity (STDP). Pairwise STDP is simple , \\nevent-driven, and highly amenable to hardware implementation. \\nFor a given synapse connecting presynaptic neuron j to \\npostsynaptic neuron i, an implementation needs only maintain \\nthe most recent spike times for the two neurons (\\u0bdd\\u0be3\\u0be5\\u0bd8 and \\u0bdc\\u0be3\\u0be2௦௧). \\nGiven a spike arrival at time t, one local nonlinear computation \\nneeds to be evaluated in order to update the synaptic weight: \\n\\u0bdc\\u0be3\\u0be2௦௧൯, On presynaptic spike\\n\\u0bdd\\u0be3\\u0be5\\u0bd8൯, On postsynaptic spike \\n( 3 ) \\nwhere )ݔis some approximation of )ݔ(ܪ for constants \\nA− < 0, A+ > 0, and τ > 0. Since a design must already perform a \\nlookup of weight wi,j on any presynaptic spike arrival, the first case \\nabove matches the natural dataflow present in any neuromorphic \\nimplementation. To support this depressive half of the STDP \\nlearning rule, the handling of a presynaptic spike arrival simp ly \\nt u r n s  a  r e a d  o f  t h e  w e i g h t  s t a t e  i n t o  a  r e a d - m o d i f y - w r i t e  \\noperation, assuming availability of the tpost spike time. \\nThe potentiating half of Equation 3 is the only significant \\nchallenge that pairwise STDP introduces. To handle this weight \\nupdate in an event-driven manner, symmetric to the depressive \\ncase, the implementation needs to perform a backwards routing \\ntable lookup, obtaining wi,j from the firing postsynaptic neuron i. \\nThis is at odds with the algorithmic impetus for more complex a nd diverse network routing functions R : j → Y , where i ∈ Y . The \\nmore complex R b e c o m e s ,  t h e  m o r e  e x p e n s i v e ,  i n  g e n e r a l ,  i t  \\nbecomes to implement an inverse lookup R−1 efficiently in \\nhardware. Some implementations have explored creative \\nsolutions to this problem [12], but in general these approaches  \\nconstrain network topologies and are not scalable. \\nFor Loihi, we adopt a less event-driven epoch-based synaptic \\nmodification architecture in the interest of supporting arbitra rily \\ncomplex R and extending the architecture to more advanced \\nlearning rules. This architecture delays the updating of all sy naptic \\nstate to the end of a periodic learning epoch time Tepoch. \\nAn epoch-based architecture fundamentally requires iteration \\nover each core’s active input axons, which Loihi does sequentia lly. \\nIn theory this is a disadvantage that a direct implementation o f the \\nR−1 reverse lookup may avoid. However, in practice, any pipelined \\ndigital core implementation stil l requires iteration over activ e \\ninput axons in order to maintain spike timestamp or trace state . \\nEven the fully transposable synaptic crossbar architecture used  in \\n[12] includes an iteration over all input axons per timestep fo r this \\nreason. \\n3.4.2 Advancing Beyond Pairwise STDP \\nA number of architectural challenges arise in the pursuit of \\nsupporting more advanced learning rules. First, the functional \\nforms describing ∆wi,j become more complex and seemingly \\narbitrary. These rules are at the frontier of algorithm researc h and \\ntherefore require a high degree of configurability. Second, the  \\nrules involve multiple synaptic variables, not just weights. Fi nally, \\nadvanced learning rules rely on temporal correlations in spikin g \\nactivity over a range of timescales, which means more than just  \\nthe most recent spike times must be maintained. These challenge s \\nmotivate the central features of Loihi’s learning architecture,  \\ndescribed below. \\n3.4.3 Learning Rule Functional Form \\nOn every learning epoch, a synapse will be updated whenever the  \\nappropriate pre- or post-synaptic conditions are satisfied. A s et of \\nmicrocode operations associated with the synapse determines the  \\nfunctional form of one or more transformations to apply to the synapse’s state variables. The rules are specified in sum-of-\\nproducts form: \\n\\u0bdc,\\u0bdd)\\u0be1\\u0cd4\\n\\u0bddୀଵேು\\n\\u0bdcୀଵ \\n \\n( 4 ) \\nwhere z is the transformed synaptic variable (either wgt, dly, or \\ntag), Vi,j refers to some choice of input variable available to the \\nlearning engine, and Ci,j and Si are microcode-specified signed \\nconstants. \\nTable 1 provides a comprehensive list of product terms as \\nencoded by a 4-bit field in each microcode op. The multiplicati ons \\nand summations of Equation 4 are computed iteratively by the hardware and accumulated in 16-bit registers. The epoch period \\nis globally configured per core u p to a maximum value of 63, wi th \\nTi,j\\nPiThis article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 6}),\n",
       " Document(page_content='7 \\ntypical values in the 2 to 8 range. To avoid receiving more tha n \\none spike in a given epoch, the epoch period is normally set to  the \\nminimum refractory delay of all neurons in the network. \\n \\nThe basic pairwise STDP rule only requires two products \\ninvolving four of these terms (0, 1, 3, and 4) and two constant s. \\nThe Loihi microcode format can specify this rule in a single 32 -bit \\nword. With an encoding capacity of up to sixteen 32-bit words a nd \\nthe full range of terms in Table 1, the learning engine provide s \\nconsiderable headroom for far more complex rules. \\n3.4.4 Trace Evaluation \\nThe trace variables ( x1,x2,y1,y2,y3,r1)  i n  T a b l e  1  r e f e r  t o  f i l t e r e d  \\nspike trains associated with each synapse that the learning eng ine \\nmodifies. The filtering function associated with each trace is \\ndefined by two configurable quantities: an impulse amount δ \\nadded on every spike event and a decay factor α. Given a spike \\narrival sequence s[t] ∈ {0,1}, an ideal trace sequence x[t] over \\ntime is defined as follows: \\n]. \\n( 5 ) \\nThe Loihi hardware computes a low-precision (seven bit) \\napproximation of this first-order  filter using stochastic round ing. \\nBy setting δ to 1 (typically with relatively small α), x[t] \\nsaturates on each spike and its decay measures elapsed time sin ce \\nthe most recent spike. Such trace configurations exactly implement the baseline STDP rules dependent only on nearest-\\nneighbor pre/post spike time separations described in Section \\n3.4.1. On the other hand, setting \\nδ t o  a  v a l u e  l e s s  t h a n  1 ,  \\nspecifically 1 − αTmin , where Tmin is the minimum spike period, \\ncauses sufficiently closely spaced spike impulses to accumulate  \\nover time and x[t] reflects the average spike rate over a timescale \\nof τ = −1 /log α. 4 D ESIGN IMPLEMENTATION \\n4.1 Core Microarchitecture \\nFigure 4 shows the internal structure of the Loihi neuromorphic  \\ncore. Colored blocks in this diagram represent the major \\nmemories that store the connectivity, configuration, and dynami c \\nstate of all neurons mapped to the core. The core’s total SRAM capacity is 2Mb including ECC overhead. The coloring of memorie s \\nand dataflow arcs illustrates th e core’s four primary operating  \\nmodes: input spike handling (green), neuron compartment updates (purple), output spike generation (blue), and synaptic \\nupdates (red). Each of these modes operates independently with \\nminimal synchronization at a variety of frequencies, based on t he \\nstate and configuration of the core. The black structure marked  \\nUCODE represents the configurable learning engine. \\nThe values annotated by each memory indicate its number of \\nlogical addresses, which correspond to the core’s major resourc e \\nconstraints. The number of input and output axons (\\nNaxin and \\nNaxout), the synaptic memory size ( Nsyn), and the total number of \\nneuron compartments ( Ncx) impose network connectivity \\nconstraints as described in Section 3.3. The parameter Nsdelay \\nindicates the minimum number of synaptic delay units supported,  \\neight in Loihi. Larger synaptic delay values, up to 62, may be supported when fewer neuron compartments are needed by a \\nparticular mapped network. \\nVarying degrees of parallelism and serialization are applied to  \\nsections of the core’s pipeline in order to balance the through put \\nbottlenecks that typical workloads will encounter. Dataflow drawn with finely dotted arrows in Figure 4 indicate parts of t he \\ndesign where single events are expanded into a potentially larg e \\nnumber of dependent events. In these areas, we generally parallelize the hardware. \\nFor example, synapses are extracted from SYNAPSE_MEM’s \\n64-bit words with up to four-way parallelism, depending on the \\nsynaptic encoding format, and that parallelism is extended to \\nDENDRITE_ACCUM and throughout the synaptic modification pipeline in the learning engine. Conversely, the presynaptic tr ace \\nstate is stored together with SY NAPSE_MEM pointer entries in th e \\nSYNAPSE_MAP memory, which then may result in multiple serial accesses per ingress spike. This balances pipeline throughputs for \\ningress learning-enabled axons when their synaptic fanout facto r \\nwithin the core is on the order of 10:1 while maintaining the b est \\npossible area efficiency. \\nRead-modify-write (RMW) memory accesses, shown as loops \\naround the relevant memories in Figure 4, are fundamental to th e \\nneuromorphic computational model and unusually pervasive \\ncompared to many other microarchitecture domains. Such loops \\ncan introduce significant design challenges, particularly for \\nperformance. We manage this challenge with an asynchronous design pattern that encapsulates and distributes the memory’s \\nstate over a collection of single-ported SRAM banks. The \\nencapsulation wrapper presents a simple dual-ported interface t o \\nthe environment logic and avoids severely stalling the pipeline  \\nexcept for statistically rare address conflicts. Encoding Term (Ti,j) Bits Description \\n0 x0 +C 5b (U)  Presynaptic spike count  \\n1 x1 +C 7b (U)  1st presynaptic trace  \\n2 x2 +C 7b (U)  2nd presynaptic trace  \\n3 y0 +C 5b (U)  Postsynaptic spike count  \\n4 y1 +C 7b (U)  1st postsynaptic trace  \\n5 y2 +C 7b (U)  2nd postsynaptic trace  \\n6 y3 +C 7b (U)  3rd postsynaptic trace  \\n7 r0 +C 1b (U)  Reward spike  \\n8 r1 +C 8b (S)  Reward trace  \\n9 wgt+C 9b (S)  Synaptic weight  \\n10 dly+C 6b (U)  Synaptic delay  \\n11 tag+C 9b (S)  Synaptic tag  \\n12 sgn(wgt+C) 1b (S)  Sign of case 9 ( ±1) \\n13 sgn(dly+C) 1b (S)  Sign of case 10 ( ±1) \\n14 sgn(tag+C) 1b (S)  Sign of case 11 ( ±1) \\n15 C 8b (S)  Constant term. (Variant 1)  \\n15 Sm · 2Se 4b (S)  Scaling term. 4b mantissa, \\n4b exponent. (Variant 2)  \\nTABLE 1: Learning rule product terms This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 7}),\n",
       " Document(page_content='8 \\n4.2 Asynchronous Design Methodology \\nBiological neural networks are fundamentally asynchronous, as \\nreflected by the absence of an explicit synchronization assumpt ion \\nin the continuous time SNN model given in Section 2. Accordingl y, \\nasynchronous design methods have long been seen as the appropriate tool for prototyping spiking neural networks in sil icon, \\nand most published chips to date use this methodology. Loihi is  no \\ndifferent and in fact the asynchronous design methodology developed for Loihi is the most advanced of its kind. \\nFor rapid neuromorphic design prototyping, we extended and \\nimproved on an earlier asynchronous design methodology used to \\ndevelop several generations of c ommercial Ethernet switches. In  \\nthis methodology, designs are entered according to a top-down decomposition process using the CAST and CSP languages. \\nModules in each level of design hierarchy communicate over \\nmessage-passing channels that ar e later mapped to a circuit-lev el \\nimplementation, which in this case is a bundled data \\nimplementation comprising a data payload with request and \\nacknowledge handshaking signals that mediate the propagation of  \\ndata tokens through the system. Figure 5 shows a template \\np i p e l i n e  e x a m p l e .  E a c h  p i p e l i n e  s t a g e  h a s  a t  l e a s t  o n e  p u l s e  \\ngenerator, such as the one shown in Figure 6, that implements t he \\ntwo-phase handshake and latch sequencing. \\nFine-grain flow control is an important property of \\nasynchronous design that offers several benefits for \\nneuromorphic applications. First, since the activity in SNNs is  \\nhighly sparse in both space and time, the activity gating that \\ncomes automatically with asynchronous flow control eliminates \\nthe power that would often be wasted by a continuously running \\nclock. Second, local flow control allows different modules in t he \\ns a m e  d e s i g n  t o  r u n  a t  t h e i r  n a t u r a l  m i c r o a r c h i t e c t u r a l  \\nfrequencies. This properly complements the need for spiking \\nneuron processes to run at a variety of timescales dependent on  \\nworkload and can significantly simplify back-end timing closure . \\nFinally, asynchronous techniques can reduce or eliminate timing  \\nmargin. In Loihi, the mesh-level barrier synchronization mechanism is the best example of asynchronous handshaking providing a globally significant performance advantage by \\neliminating needless mesh-wide idle time. \\nGiven a hierarchical design decomposition written in CSP, a \\npipeline synthesis tool converts the CSP module descriptions to  \\nV e r i l o g  r e p r e s e n t a t i o n s  t h a t  a r e  c o m p a t i b l e  w i t h  s t a n d a r d  E D A  \\ntools. The initial Verilog representation supports logic synthe sis to \\nboth synchronous and asynchronous implementations with full functional equivalence, providing support for synchronous FPGA \\nemulation of the design. \\nFig. 4: Core Top-Level Microarchitecture. The SYNAPSE unit proc esses all incoming spikes and reads out the associated synaptic  weights from the memory. \\nThe DENDRITE unit updates the st ate variables u and v of all ne urons in the core. The AXON unit generates spike messages for a ll fanout cores of each firing \\nneuron. The LEARNING unit update s synaptic weights using the pr ogrammed learning rule s at epoch boundaries.  \\nL \\nA \\nT \\nC \\nH \\nPULSE \\nGENERATOR  \\nEN \\n DEN \\nL \\n. \\nq \\n R \\n. \\nq \\nR \\n. \\na \\n L \\n. \\na \\nPulse \\n- \\nwidth  \\nextension  \\nPipeline  \\n p\\ndatapath  \\nDATA \\nIN \\nDATA \\nOUT \\nL \\nA \\nT \\nC \\nH \\nL \\nA \\nT \\nC \\nH \\nLogic \\nFig. 5: Bundled data pipeline stage \\nEN \\n DEN \\nR \\n. \\nq \\nR \\n. \\na \\nL \\nL\\n. \\na \\nL \\nL\\n. \\nq \\n LATCH \\n D \\n Q \\nPULSE \\nGENERATOR  \\nFig. 6: Bundled data pulse generator circuit This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 8}),\n",
       " Document(page_content='9 \\nThe asynchronous back-end layout flow uses standard tools \\nwith an almost fully standard cell library. Here, the asynchron ous \\nmethodology simplifies the layout closure problem. At every lev el \\nof layout hierarchy, all timing constraints apply only to neighboring, physically proximate pipeline stages. This greatly  \\nfacilitates convergent timing closure, especially at the chip l evel. \\nFor example, the Loihi mesh assembles by physical abutment \\nwithout needing any unique clock distribution layout or timing \\nanalysis for different mesh dimensions or core types. \\n5 R ESULTS \\n5.1 Silicon Realization \\nLoihi was fabbed in Intel’s 14nm FinFET process. The chip \\ninstantiates a total of 2.07 billion transistors and 33 MB of S RAM \\nover its 128 neuromorphic cores an d three x86 cores, with a die  \\narea of 60 mm2. The device is functional over a supply voltage \\nrange of 0.50V to 1.25V. Table 2 provides a selection of energy  and \\nperformance measurements from pre-silicon SDF and SPICE \\nsimulations, consistent with early post-silicon characterizatio n. \\nLoihi includes a total of 16MB of synaptic memory. With its \\ndensest 1-bit synapse format, this provides a total of 2.1 mill ion \\nunique synaptic variables per mm2, over three times higher than \\nTrueNorth, the previously most dense SNN chip [11]. This does n ot \\nconsider Loihi’s hierarchical network support that can signific antly \\nb o o s t  i t s  e f f e c t i v e  s y n a p t i c  d e n s i t y .  O n  t h e  o t h e r  h a n d ,  L o i h i ’ s \\nmaximum neuron density of 2,184 per mm2 is marginally worse \\nthan TrueNorth’s. Process normalized, this represents a 2× \\nreduction in the design’s neuron density, which may be \\ninterpreted as the cost of Loihi’s greatly expanded feature set , an \\nintentional design choice. \\n5.2 Algorithmic Results \\nOn an earlier iteration of the Loihi architecture, we quantitat ively \\nassessed the efficiency of Spiking LCA to solve LASSO, as descr ibed \\nin Section 2.2. We used a 1.67 GHz Atom CPU running both LARS and FISTA [3] numerical solvers as a reference architecture for  \\nbenchmarking. These solvers are among the best known for this \\nproblem. Both chips were fabbed in 14nm technology, were \\nevaluated at a 0.75V supply voltage, and required similar activ e \\nsilicon areas (5 mm2). \\nMeasured parameter Value at 0.75V \\nCross-sectional spike bandwidth per tile  3.44 Gspike/s  \\nWithin-tile spike energy  1.7 pJ  \\nWithin-tile spike latency  2.1 ns  \\nEnergy per tile hop (E-W / N-S)  3.0 pJ / 4.0 pJ  \\nLatency per tile hop (E-W / N-S)  4.1 ns / 6.5 ns  \\nEnergy per synaptic spike op (min)  23.6 pJ  \\nTime per synaptic spike op (max)  3.5 ns  \\nEnergy per synaptic update (pairwise STDP)  120 pJ  \\nTime per synaptic update (pairwise STDP)  6.1 ns  \\nEnergy per neuron update (active / inactive)  81 pJ / 52 pJ  \\nTime per neuron update (active / inactive)  8.4 ns / 5.3 ns  \\nMesh-wide barrier sync time (1-32 tiles)  113-465ns  \\nTABLE 2: Loihi pre-silicon performance and energy measurements \\n \\n \\n(a) Original  \\n(b) Reconstruction \\nFig. 8: Image reconstruction f rom the sparse coefficients \\ncomputed using the Loihi predecessor.  \\nThe largest problem we evaluated is a convolutional sparse \\ncoding problem on a 52 ×52 image with a 224-atom dictionary, a \\npatch size of 8 ×8, and a patch stride of 4 pixels. Loihi’s hierarchical \\nconnectivity provided a factor of 18 compression in synaptic \\nresources for this network. We solved the sparse coding problem  \\nto a solution within 1% of the optimal solution. Figure 8 compa res \\nt h e  o r i g i n a l  a n d  t h e  r e c o n s t r u c t e d  i m a g e  u s i n g  t h e  c o m p u t e d  \\nsparse coefficients. \\nTable 3 shows the comparison in computational efficiency \\nbetween these two architectures, as measured by EDP. It is not surprising to see that the conventional LARS solver can handle \\nproblems of small sizes and very sparse solutions quite efficie ntly. \\nOn the other hand, the conventional solvers do not scale well f or \\nthe large problem and the Loihi predecessor achieves the target  \\nobjective value with over 5,000 times lower EDP. \\n \\n \\n \\n \\n  \\n \\nNo. Unknowns 400 1,700 32,256 \\nNo. nonzeros in solutions  ≈10 ≈30 ≈420 \\nEnergy  2.58x 8.08x 48.74x  \\nDelay 0.27x 2.76x 118.18x  \\nEDP 0.7x 22.33x  5760x  \\nTABLE 3: Comparison of solving ℓ1 minimization on Loihi and Atom. \\nResults are expressed as improvement ratios Atom/Loihi. The Ato m \\nnumbers are chosen from using th e more efficient solver between  LARS \\nand FISTA. \\nFig. 7: Loihi chip plot This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 9}),\n",
       " Document(page_content='10 \\nLoihi’s flexible learning engine allows one to explore and \\nexperiment with various learning methods. We have developed \\na n d  v a l i d a t e d  t h e  f o l l o w i n g  n e t w o r k s  i n  p r e - s i l i c o n  F P G A  \\nemulation with all learning taking place on chip: \\n• A single-layer classifier using a supervised variant of STDP \\nsimilar to [4] as the learning method. This network, when \\ntrained with local-intensity-change based temporally spike-coded image samples, can achieve 96% accuracy on \\nthe MNIST dataset using ten neurons, in line with a \\nreference ANN of the same structure. \\n• Solving the shortest path problem of a weighted graph. \\nVertices and edges are represented as neurons and \\nsynapses respectively. The algorithm is based on the \\neffects of STDP on a propagating wavefront of spikes [13]. \\n• Solving a one-dimensional, non-Markovian sequential \\ndecision making problem. The network learns the decision making policy in response to delayed reward and \\npunishment feedback similar to [14]. \\nThe algorithmic development and characterization of Loihi is \\njust beginning. These proof-of-concept examples use only a fraction of the resources and features available in the chip. W ith \\nLoihi now in hand, our focus turns to scaling and further evalu ating \\nthese networks. \\n6 C ONCLUSION \\nLoihi is Intel’s fifth and most complex fabricated chip in a fa mily of \\ndevices that explore different points in the neuromorphic desig n \\nspace spanning architectural variations, circuit methodologies,  \\nand process technology. In some respects, its flexibility may g o too \\nfar, while in others, not far enough. Further optimizations of the \\narchitecture and implementation are planned. The pursuit of \\ncommercially viable neuromorphic architectures and algorithms \\nmay well end at design points far from what we have described i n \\nthis paper, but we hope Loihi provides a step in the right dire ction. \\nWe offer it as a vehicle for coll aborative exploration with the  \\nbroader research community. \\nREFERENCES \\n[1] P. T. P. Tang, T.-H. Lin, and M. Davies, “Sparse coding by spik ing neural \\nnetworks: Convergence theory and computational results,” arXiv e-\\nprints , 2017.  \\n[2] S. Shapero, M. Zhu, J. Hasler, and C. Rozell, “Optimal sparse \\napproximation with integrate and fire neurons,” International journal of \\nneural systems , vol. 24, no. 5, p. 1440001, 2014.  \\n[3] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholdi ng \\nalgorithm for linear inverse problems,” SIAM Journal on Imaging \\nSciences , vol. 2, no. 1, pp. 183–202, 2009.  \\n[4] F. Ponulak and A. Kasinski, “Supervised learning in spiking neu ral´ \\nnetworks with ReSuMe: sequence learning, classification, and sp ike \\nshifting,” Neural Computation , vol. 22, no. 2, pp. 467–510, 2010.  \\n[5] T.-H. Lin, “Local Information wi th Feedback Perturbation Suffic es for \\nDictionary Learning in  Neural Circuits,” arXiv e-prints , 2017.  \\n[6] E. Neftci, C. Augustine, S. Paul, and G. Detorakis, “Event-Driv en Random \\nBack-Propagation: Enabling Neuro morphic Deep Lea rning Machines, ” \\nFrontiers in neuroscience , vol. 11, p. 324, 2017.  \\n[7] J. Gjorgjieva, C. Clopath, J. Audet, and J.-P. Pfister, “A trip let spike-\\ntiming dependent plasticity model generalizes the Bienenstock C ooper \\nMunro rule to higher-order s patiotemporal correlations,” Proceedings \\nof the National Academy of Sciences , vol. 108, no. 48, pp. 19 383–19 \\n388, 2011.  [8] N. Qiao, H. Mostafa, F. Corradi, M. Osswald, F. Stefanini, D. \\nSumislawska, and G. Indiveri, “A reconfigurable on-line learnin g spiking \\nneuromorphic processor comprising 256 neurons and 128K synapses ,” \\nFrontiers in Neuroscience , vol. 9, p. 141, 2015.  \\n[9] L. Buesing, J. Bill, B. Nessler, and W. Maass, “Neural dynamics  as \\nsampling: a model for stochastic computation in recurrent netwo rks of \\nspiking neurons,” PLoS computational biology , vol. 7, no. 11, p. \\ne1002211, 2011.  \\n[10] E. M. Izhikevich, “Polychronizat ion: Computation with Spikes,” Neural \\nComputation , vol. 18, no. 2, pp. 245–282, 2006, pMID: 16378515.  \\n[11] P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy,  \\nJ. Sawada, F. Akopyan, B. L. Jack son, N. Imam, C. Guo, Y. Nakam ura, B. \\nBrezzo, I. Vo, S. K. Esser, R. Appuswamy, B. Taba, A. Amir, M. D. \\nFlickner, W. P. Risk, R. Manohar, and D. S. Modha, “A million s piking-\\nneuron integrated circuit with a scalable communication network  and \\ninterface,” Science , vol. 345, no. 6197, pp. 668–673, 2014.  \\n[12] J. s. Seo, B. Brezzo, Y. Liu, B. D. Parker, S. K. Esser, R. K. Montoye, B. \\nRajendran, J. A. Tierno, L. Chang, D. S. Modha, and D. J. Fried man, “A \\n45nm CMOS neuromorphic chip with a scalable architecture for \\nlearning in networks of spiking neurons,” in 2011 IEEE Custom \\nIntegrated Circuits Conference (CICC) , Sept 2011, pp. 1–4.  \\n[13] F. Ponulak and J. J. Hopfield, “Rapid, parallel path planning b y \\npropagating wavefronts of spiking neural activity,” Frontiers in \\nComputational Neuroscience , vol. 7, p. 98, 2013.  \\n[14] R. V. Florian, “Reinforcement le arning through modulation of sp ike-\\ntiming-dependent synaptic plasticity,” Neural Computation , vol. 19, no. \\n6, pp. 1468–1502, 2007.  \\n \\n \\nAUTHOR  INFORMATION  \\nA t  t h e  t i m e  o f  d e v e l o p m e n t ,  a l l  a u t h o r s  w e r e  r e s e a r c h e r s  i n  I n t el Labs’ \\nArchitecture and Design Research (ADR) division of Intel Labs.  Loihi chip \\ndevelopment and algorithms research was performed in the Microa rchitecture \\nResearch Lab (MRL) headed by Hong Wang , Intel Fellow.  Mike Davies  l e d  \\nsilicon development, Narayan Srinivasa  led algorithms research and \\narchitectural modeling.  Tsung-Han Lin  is a researcher in MRL focused on \\nsparse coding and related learning algorithms.  Gautham Chinya , also in MRL \\nfocused on advanced IP prototyping, led validation and SDK deve lopment.  \\nGeorgios Dimou , Prasad Joshi , Andrew Lines , Ruokun Liu , Steve McCoy , \\nJonathan Tse , and Yi-Hsin Weng  developed Loihi’s async hronous architecture, \\ndesign flow, and design components, and Sri Harsha Choday  contributed to \\nasynchronous circuit validation.  Yongqiang Cao , Nabil Imam , Arnab Paul , and \\nAndreas Wild  contributed to Loihi’s algorithms, feature set, and modeling.   \\nShweta Jain , Chit-Kwan Lin , Deepak Mathaikutty , Guruguhanathan \\nVenkataramanan , and Yoonseok Yang  prototyped proof-of-concept networks \\nand software to demonstrate the chip’s learning capabilities an d validate its \\nfunctionality, and also provided synchronous and FPGA design de velopment \\nsupport.  Yuyun Liao , a silicon implementation manager in ADR, helped to \\nvalidate all aspects of the final Loihi layout implementation.  Going forward, \\nMike Davies  leads all ongoing neuromorphic research in Intel Labs as head of \\nits Neuromorphic Computing Lab.   Any inquiries should be direct ed to him. This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE\\nView publication statsView publication stats', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents = loader.load()\n",
    "\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/322548911\\nLoihi: A Neu romorphic Manycore Processor with On-Chip Learning\\nArticle \\xa0\\xa0 in\\xa0\\xa0IEEE Micr o · Januar y 2018\\nDOI: 10.1109/MM.2018.112130359\\nCITATIONS\\n1,153READS\\n20,604\\n9 author s, including:\\nSome o f the author s of this public ation ar e also w orking on these r elat ed pr ojects:\\nLoihi neur omorphic pr ocessor with on-chip le arning  View pr oject\\nSilic on spin qubits  View pr oject\\nMike Davies\\nIntel\\n16 PUBLICA TIONS \\xa0\\xa0\\xa01,491  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nNarayan Sriniv asa\\nIntel\\n5 PUBLICA TIONS \\xa0\\xa0\\xa01,276  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAndr ew Lines\\nIntel\\n22 PUBLICA TIONS \\xa0\\xa0\\xa02,012  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAndr eas Wild\\nIntel\\n15 PUBLICA TIONS \\xa0\\xa0\\xa01,473  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Mike Davies  on 04 A ugust 2018.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 0}),\n",
       " Document(page_content='1 \\nLoihi: a Neuromorphic Manycore Processor  with \\nOn-Chip Learning  \\nMike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday,  \\nGeorgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, Yuyun Li ao, Chit-Kwan Lin, Andrew Lines,  \\nRuokun Liu, Deepak Mathaikutty, Steve McCoy, Arnab Paul, Jonath an Tse,  \\nGuruguhanathan Venkatar amanan, Yi-Hsin Weng , Andreas Wild, Yoon seok Yang, Hong Wang  \\nContact: mike.davies@intel.com  \\nIntel Labs, Intel Corporation \\nAbstract —Loihi is a 60 mm2 chip fabricated in Intel’s 14nm process that advances the state -of-the-art modeling of spiking neural networks in \\nsilicon. It integrates a wide range of novel features for the f ield, such as hierarchical conne ctivity, dendritic compartments , synaptic delays, and \\nmost importantly programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorit hm, Loihi can solve', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 1}),\n",
       " Document(page_content='most importantly programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorit hm, Loihi can solve \\nLASSO optimization problems with over three orders of magnitude  superior energy-delay-product compared to conventional solvers  running on a \\nCPU iso-process/voltage/area. This provides an unambiguous exam ple of spike-based computation outperforming all known conventi onal \\nsolutions. \\nKeywords —Neural nets, Neuromorphic computing, Artificial Intelligence, M achine learning, Computing Methodologies, Other Architecture St yles \\n\\x8b \\n1 I NTRODUCTION \\nEUROSCIENCE offers a bountiful source of inspiration for novel \\nhardware architectures and algorithms. Through their \\ncomplex interactions at large sca les, biological neurons exhibi t an \\nimpressive range of behaviors and properties that we currently struggle to model with modern analytical tools, let alone repli cate \\nwith our design and manufacturing technology. Some of the magic', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 1}),\n",
       " Document(page_content='impressive range of behaviors and properties that we currently struggle to model with modern analytical tools, let alone repli cate \\nwith our design and manufacturing technology. Some of the magic  \\nthat we see in the brain undoubtedly stems from exotic device a nd \\nmaterial properties that will remain out of our fabs’ reach for  \\nmany years to come. Yet highly simplified abstractions of neura l \\nne t w ork s  a re  now  re vol u t i o ni z i n g  computing by solving difficult  \\nand diverse machine learning problems of great practical value.  \\nPerhaps other less simplified models may also yield near-term \\nvalue. \\nArtificial neural networks (ANNs) are reasonably well served \\nb y  t o d a y ’ s  v o n  N e u m a n n  C P U  a r c h i t e c t u r e s  a n d  G P U  v a r i a n t s ,  especially when assisted by coprocessors optimized for streamin g \\nmatrix arithmetic. Spiking neural network models, on the other', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 1}),\n",
       " Document(page_content='matrix arithmetic. Spiking neural network models, on the other \\nhand, are exceedingly poorly served by conventional architectures. Just as the value of ANNs was not fully apprecia ted \\nuntil the advent of sufficiently fast CPUs and GPUs, the same c ould \\nbe the case for spiking models—except different computing \\narchitectures will be required. \\nThe neuromorphic computing field of research spans a range \\nof different neuron models and levels of abstraction. Loihi \\n(pronounced ”low-EE-hee”) is one stake in the ground motivated \\nby a particular class of algorithmic results and perspectives f rom \\nour survey of computational neuroscience and recent \\nneuromorphic advances. We approach the field with an eye for \\nm a t h e m a t i c a l  r i g o r ,  t o p - d o w n  m o d e l i n g ,  r a p i d  a r c h i t e c t u r e  \\niteration, and quantitative benchmarking. Our aim is to develop', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 1}),\n",
       " Document(page_content='m a t h e m a t i c a l  r i g o r ,  t o p - d o w n  m o d e l i n g ,  r a p i d  a r c h i t e c t u r e  \\niteration, and quantitative benchmarking. Our aim is to develop  \\nalgorithms and hardware in a principled way as much as possible . W e  b e g i n  t h i s  p a p e r  w i t h  o u r  d e f i n i t i o n  o f  t h e  S N N  \\ncomputational model and the features that motivated Loihi’s architectural requirements. We then describe the architecture that supports those requirements and provide an overview of the  \\nchip’s asynchronous design implementation. We conclude with \\nsome preliminary 14nm silicon results. \\nImportantly, Section 2.2 presents a result that unambiguously \\ndemonstrates the value of spike-based computation for one \\nfoundational problem. We view this as a significant result in l ight \\nof ongoing debate about the value of spikes as a computational tool in both mainstream and neuromorphic communities. The', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 1}),\n",
       " Document(page_content='foundational problem. We view this as a significant result in l ight \\nof ongoing debate about the value of spikes as a computational tool in both mainstream and neuromorphic communities. The \\nskepticism towards spikes is well founded, but in our research we \\nhave moved on from this question, given the existence of an example that potentially generalizes to a very broad class of \\nneural networks, namely all recurrent networks. \\n2 S PIKING NEURAL NETWORKS \\nW e  c o n s i d e r  a  s p i k i n g  n e u r a l  n e t w o r k  ( S N N )  a s  a  m o d e l  o f  \\ncomputation with neurons as the basic processing elements. \\nDifferent from artificial neural networks, SNNs incorporate tim e \\nas an explicit dependency in their computations. At some instan t \\nin time, one or more neurons may send out single-bit impulses, \\nthe spike , to neighbors through directed connections known as \\nsynapses , with a potentially nonzero traveling time. Neurons have', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 1}),\n",
       " Document(page_content='in time, one or more neurons may send out single-bit impulses, \\nthe spike , to neighbors through directed connections known as \\nsynapses , with a potentially nonzero traveling time. Neurons have \\nlocal state variables with rules governing their evolution and \\ntiming of spike generation. Hence the network is a dynamical \\nsystem where individual neurons interact through spikes. N This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 1}),\n",
       " Document(page_content='2 \\n2.1 Spiking Neural Unit \\nA spiking neuron integrates its spike train input in some fashi on, \\nusually by low pass filter, and fires once a state variable exc eeds a \\nthreshold. Mathematically, each spike train is a sum of Dirac d elta \\nfunctions \\u0bde) \\u0bde  where tk i s  t h e  t i m e  o f  t h e  k-th \\nspike. We adopt a variation of the well-known CUBA leaky- integrate-and-fire model that has two internal state variables,  the \\nsynaptic response current \\nui(t) and the membrane potential vi(t).  \\nThe synaptic response current is the sum of filtered input spik e \\ntrains and a constant bias current: \\n \\n\\u0bdc\\n\\u0bddஷ\\u0bdc \\n( 1 ) \\nwhere wij i s  t h e  s y n a p t i c  w e i g h t  f r o m  n e u r o n - j to i, =)ݐ\\n)ݐ(ܪis the synaptic filter impulse response \\nparameterized by the time constant τu with H(t) the unit step \\nfunction, and bi is a constant bias. The synaptic current is further \\nintegrated as the membrane potential, and the neuron sends out', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       " Document(page_content='parameterized by the time constant τu with H(t) the unit step \\nfunction, and bi is a constant bias. The synaptic current is further \\nintegrated as the membrane potential, and the neuron sends out \\na spike when its membrane potential passes its firing threshold  θi. \\n \\n−1\\n௩)ݐ \\n2 ) \\nNote that the integration is leaky, as captured by the time \\nconstant τv. vi is initialized with a value less than θi, and is reset to \\n0 right after a spiking event occurs. \\nLoihi, a fully digital architecture, approximates the above \\ncontinuous time dynamics using a fixed-size discrete timestep \\nmodel. In this model, all neurons need to maintain a consistent  \\nunderstanding of time so their distributed dynamics can evolve in \\na well-defined, synchronized manner. It is worth clarifying tha t \\nthese fixed-size, synchronized time steps relate to the algorithmic time of the computation, and need not have a direct relationship \\nto the hardware execution time.', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       " Document(page_content='these fixed-size, synchronized time steps relate to the algorithmic time of the computation, and need not have a direct relationship \\nto the hardware execution time.   \\n2.2 Computation with Spikes and Fine-grained Parallelism \\nC o m p u t a t i o n s  i n  S N N s  a r e  c a r r i e d  o u t  t h r o u g h  t h e  i n t e r a c t i n g  \\ndynamics of neuron states. An instructive example is the ℓ1-\\nminimizing sparse coding problem, also known as LASSO, which \\nwe can solve with the SNN in Figure 1a using the Spiking Locall y \\nCompetitive Algorithm [2]. The objective of this problem is to determine a sparse set of coefficients that best represents a g iven \\ninput as the linear combination of features from a feature \\ndictionary. The coefficients can be viewed as the activities of  the \\nspiking neurons in Figure 1a that are competing to form an \\naccurate representation of the d ata. By properly configuring th e', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       " Document(page_content='dictionary. The coefficients can be viewed as the activities of  the \\nspiking neurons in Figure 1a that are competing to form an \\naccurate representation of the d ata. By properly configuring th e \\nn e t w o r k ,  i t  c a n  b e  e s t a b l i s h e d  t h a t  a s  t h e  n e t w o r k  d y n a m i c s  evolve, the average spike rates of the neurons will converge to a \\nfixed point, and this fixed point is identical to the solution of the \\noptimization problem. \\nSuch computation exhibits completely different characteristics \\nfrom conventional linear algebra based approaches. Figure 1b compares the computational efficiency of an SNN with the \\nconventional solver FISTA [3] by having them both solve a spars e \\ncoding problem on a single-threaded CPU. The SNN approach (labelled S-LCA) gives a rapid initial drop in error and obtain s a \\ngood approximate solution faster than FISTA. After this, the S- LCA', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       " Document(page_content='coding problem on a single-threaded CPU. The SNN approach (labelled S-LCA) gives a rapid initial drop in error and obtain s a \\ngood approximate solution faster than FISTA. After this, the S- LCA \\nconvergence speed significantly slows down, and FISTA instead finds a much more precise solution quicker. Hence an interestin g \\nefficiency-accuracy tradeoff arises that makes the SNN solution  \\nparticularly attractive for applications that do not require hi ghly \\nprecise solutions, e.g., a solut ion that is 1% within the optim al. \\nThe remarkable algorithmic efficiency of S-LCA can be \\nattributed to its ability to exploit the temporal ordering of s pikes, \\na general property of the SNN co mputational model. In Figure 1a , \\nthe neuron that has the largest external input to win the competition is more likely to spike at the earliest time, causi ng \\nimmediate inhibition of the other neurons. This inhibition', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       " Document(page_content='the neuron that has the largest external input to win the competition is more likely to spike at the earliest time, causi ng \\nimmediate inhibition of the other neurons. This inhibition \\nhappens with only a single one-to-many spike communication, in contrast to the usual need for all-to-all state exchanges with \\nmatrix arithmetic based solutions such as FISTA and other \\nconventional solvers. This implies that the SNN solution is c o m m u n i c a t i o n  e f f i c i e n t ,  a n d  i t  m a y  s o l v e  t h e  o p t i m i z a t i o n  \\nproblem with a reduced number of arithmetic operations. We \\npoint interested readers to [1] for more discussions. \\nOur CPU-based evaluation has yet to exploit one important \\nadvantage of SNN-based algorithms: the inherent abundant parallelism. The dominant part of SNN computations—the \\nevolution of individual neuron states within a timestep—can all  be', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       " Document(page_content='advantage of SNN-based algorithms: the inherent abundant parallelism. The dominant part of SNN computations—the \\nevolution of individual neuron states within a timestep—can all  be \\ncomputed concurrently. However, harnessing such speedup can be a nontrivial task especially on a conventional CPU architect ure. \\nThe parallelizable work for each neuron only consists of a few \\nvariable updates. Given that the  parallel segment of the work c an \\nbe executed very quickly, the underlying architecture must \\nsupport a fine granularity of parallelism with minimal overhead  in \\ncoordinating the order of computations. These observations \\nmotivate fundamental features of the Loihi architecture, \\ndescribed in Section 3. \\n…. \\nInhibitory weights \\nܾ \\n1 \\nܾ \\n2 \\nܾ \\nܰ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n  \\n \\n \\n \\n  \\n \\n  \\n \\n  \\n \\n \\n \\na) (b) \\nFig. 1 : (a) The network topology for solving LASSO. Each neuron \\nreceives the correlation bi between the input data and a predefined', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       " Document(page_content='a) (b) \\nFig. 1 : (a) The network topology for solving LASSO. Each neuron \\nreceives the correlation bi between the input data and a predefined \\nfeature vector as its input. Bottom figure shows the evolution of \\nmembrane potential in a 3- neuron example; the spike rates of the \\nneurons stabilizes to fixed values. (b) Algorithmic efficiency \\ncomparison of a solution based on spiking network (S- LCA) and \\nconventional opti mization methods (FISTA). Both algorithms are \\nimplemented on a CPU with single thread. Y- axis is the normalized \\ndifference to the optimal objective function value. Figures tak en from \\n[1] with detailed information therein. This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       " Document(page_content='3 \\n2.3 Learning with Local Information \\nLearning in an SNN refers to adapting the synaptic weights and \\nhence varying the SNN dynamics to a desired one. Similar to \\nconventional machine learning, we wish to express learning as t he \\nminimization of a particular loss function over many training samples. In the sparse coding case, learning involves finding t he \\nset of synaptic weights that allows the best performing sparse \\nrepresentation, expressed as minimizing the sum of all sparse coding losses. Learning in an SNN naturally proceeds in an onli ne \\nmanner, where training samples are sent to the network \\nsequentially. \\nSNN synaptic weight adaptation rules must satisfy a locality \\nconstraint: each weight can only  be accessed and modified by th e \\ndestination neuron, and the rule can only make use of locally \\navailable information, such as the spike trains from the \\npresynaptic (source) and postsynaptic (destination) neurons. Th e', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 3}),\n",
       " Document(page_content='destination neuron, and the rule can only make use of locally \\navailable information, such as the spike trains from the \\npresynaptic (source) and postsynaptic (destination) neurons. Th e \\nlocality constraint imposes a significant challenge on the desi gn of \\nlearning algorithms, as most conventional optimization \\nprocedures do not satisfy it. Although the development of such decentralized learning algorithms is still in active research, some \\npioneering work exists showing the promise of this approach. \\nThey range from the simple Oja’s rule for finding principal components, to the Widrow-Hoff rule for supervised learning and  \\nits generalization to exploit precise spike timing information [4], \\nto the more complex unsupervised sparse dictionary learning using feedback [5] and event-driven random back-propagation \\n[6]. \\nOnce a learning rule satisfies the locality constraint, the \\ninherent parallelism offered by SNNs will then allow the adapti ve', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 3}),\n",
       " Document(page_content='[6]. \\nOnce a learning rule satisfies the locality constraint, the \\ninherent parallelism offered by SNNs will then allow the adapti ve \\nnetwork to be scaled up to large sizes in a way that can be computed efficiently. If the rule also minimizes a loss functio n, \\nthen the system will have well defined dynamics. \\nTo support the development of such scalable learning rules, \\nLoihi offers a variety of local information to a programmable \\nsynaptic learning process: \\n• Spike traces corresponding to filtered presynaptic and \\npostsynaptic spike trains with configurable time constants \\n(Section 3.4.4). In particular, a short time constant allows t h e  l e a r n i n g  r u l e  t o  u t i l i z e  p r e c i s e  s p i k e  t i m i n g  \\ni n f o r m a t i o n ,  w h i l e  a  l o n g  t i m e  c o n s t a n t  c a p t u r e s  t h e  \\ninformation in spike rates. \\n• Multiple spike traces for a given spike train filtered with', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 3}),\n",
       " Document(page_content='i n f o r m a t i o n ,  w h i l e  a  l o n g  t i m e  c o n s t a n t  c a p t u r e s  t h e  \\ninformation in spike rates. \\n• Multiple spike traces for a given spike train filtered with \\ndifferent time constants. This provides support for differential Hebbian learning by measuring perturbations \\nin spike patterns and Bienenstock-Cooper-Munro learning \\nusing triplet STDP [7], among others. \\n• Two additional state variables per synapse, besides the \\nn o r m a l  w e i g h t ,  i n  o r d e r  t o  p r o v i d e  m o r e  f l e x i b i l i t y  f o r  learning. For example, these can be used as synaptic tags \\nfor reinforcement learning. \\n• Reward traces that correspond to special reward spikes \\ncarrying signed impulse values to represent reward or \\npunishment signals for reinforcement learning. Reward \\nspikes are broadcast to defined sets of synapses in the \\nnetwork that may connect to many different source and', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 3}),\n",
       " Document(page_content='punishment signals for reinforcement learning. Reward \\nspikes are broadcast to defined sets of synapses in the \\nnetwork that may connect to many different source and \\ndestination neurons. Loihi is the first fully integrated digital SNN chip that suppo rts any \\nof the above features. Some small-scale neuromorphic chips with  \\nanalog synapse and neuron circuits have prototyped synaptic \\nplasticity using spike traces, for example [8], but these prior  chips \\nha ve  orde rs  of  m a g ni t ude  l ow e r ne t w ork  c a pa c it y  c om pa re d t o \\nLoihi as well as far less programmability. \\n2.4 Other Computational Primitives \\nLoihi includes several computational primitives related to othe r \\nactive areas of SNN algorithmic research: \\n• Stochastic noise. Uniformly distributed pseudorandom \\nnumbers may be added to a neuron’s synaptic response current, membrane voltage, and refractory delay. This \\nprovides support for algorithms such as Neural Sampling', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 3}),\n",
       " Document(page_content='numbers may be added to a neuron’s synaptic response current, membrane voltage, and refractory delay. This \\nprovides support for algorithms such as Neural Sampling \\n[9], which can solve probabili stic inference and constraint \\nsatisfaction problems using stochastic dynamics and a \\nform of Markov chain Monte Carlo sampling. \\n• Configurable and adaptable synaptic, axon, and refractory \\ndelays. This provides support for novel forms of temporal \\ncomputation such as polychronous dynamics [10], in \\nwhich subsets of neurons may synchronize over periods of \\nvarying timescales. The number of polychronous groups \\nfar exceeds the number of stable attractors in conventional attractor networks, suggesting a productive \\nspace for computational development. \\n• Configurable dendritic tree processing. Neurons in the \\nSNN may be decomposed into a tree of compartment \\nunits, with the neuron’s input synapses distributed over those compartments. Each compartment supports the', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 3}),\n",
       " Document(page_content='SNN may be decomposed into a tree of compartment \\nunits, with the neuron’s input synapses distributed over those compartments. Each compartment supports the \\nsame state variables as a neuron, but only the root of the \\ntree (soma compartment) generates spike outputs. The compartments’ state variables are combined in a \\nconfigurable manner by programming different join \\nfunctions for each compartment junction. \\n• Neuron threshold adaptation in support of intrinsic \\nexcitability homeostasis. \\n• Scaling and saturation of synaptic weights in support of \\n“permanence” levels that exceed the range of weights used during inference. \\nThe combination of these features in one device, especially in \\ncombination with Loihi’s learning capabilities, is novel for th e field \\nof SNN silicon implementation. \\n3 A RCHITECTURE \\n3.1 Chip Overview \\nLoihi features a manycore mesh comprising 128 neuromorphic', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 3}),\n",
       " Document(page_content='of SNN silicon implementation. \\n3 A RCHITECTURE \\n3.1 Chip Overview \\nLoihi features a manycore mesh comprising 128 neuromorphic \\ncores, three embedded x86 processor cores, and off-chip communication interfaces that hierarchically extend the mesh in  \\nfour planar directions to other chips. An asynchronous network-\\non-chip (NoC) transports all communication between cores in the  \\nform of packetized messages. The NoC supports write, read \\nrequest, and read response messages for core management and \\nx86-to-x86 messaging, spike messa ges for SNN computation, and \\nbarrier messages for time synchronization between cores. All \\nmessage types may be sourced externally by a host CPU or on-chi p  This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 3}),\n",
       " Document(page_content='4 \\nby the x86 cores, and these may be directed to any on-chip core . \\nMessages may be hierarchically encapsulated for off-chip \\ncommunication over a second-level network. The mesh protocol \\nsupports scaling to 4096 on-chip cores and, via hierarchical addressing, up to 16,384 chips. \\nEach neuromorphic core implements 1,024 primitive spiking \\nneural units (compartments) grouped into sets of trees \\nconstituting neurons.  The compartments, along with their fanin  \\nand fanout connectivity, share configuration and state variable s in \\nten architectural memories. Their state variables are updated i n a \\ntime-multiplexed, pipelined manner every algorithmic timestep. \\nWhen a neuron’s activation exceeds some threshold level, it generates a spike message that is routed to a set of fanout \\ncompartments contained in some number of destination cores. \\nFlexible and well provisioned SNN connectivity features are \\ncrucial for supporting a broad range of workloads. Some desirab le', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 4}),\n",
       " Document(page_content='compartments contained in some number of destination cores. \\nFlexible and well provisioned SNN connectivity features are \\ncrucial for supporting a broad range of workloads. Some desirab le \\nnetworks may call for dense, all-to-all connectivity while othe rs \\nmay call for sparse connectivity; some may have uniform graph \\ndegree distributions, others power law distributions; some may \\nrequire high precision synaptic weights, e.g. to support learning, \\nwhile others can make do with binary connections. As a rule, \\nalgorithmic performance scales with increasing network size, \\nmeasured not only by neuron counts but especially neuron-to-neuron fanout degrees. We see this rule holding all the way to \\nbiological levels (1:10,000). Due to the \\nO(N2) scaling of \\nconnectivity state in the number of fanouts, it becomes an \\nenormous challenge to support networks with high connectivity \\nusing today’s integrated circuit technology. \\nTo address this challenge, Loihi supports a range of features t o', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 4}),\n",
       " Document(page_content='enormous challenge to support networks with high connectivity \\nusing today’s integrated circuit technology. \\nTo address this challenge, Loihi supports a range of features t o \\nrelax the sometimes severe const raints that other neuromorphic \\ndesigns have imposed on the programmer: \\n1) Sparse network compression. Besides a common dense \\nmatrix connectivity model, Loihi supports three sparse \\nm a t r i x  c o m p r e s s i o n  m o d e l s  i n  w h i c h  f a n o u t  n e u r o n  \\nindices are computed based on index state stored with each synapse’s state variables. \\n2) Core-to-core multicast. Any neuron may direct a single \\nspike to any number of destination cores, as the network \\nconnectivity may require. \\n3) Variable synaptic formats. Loihi supports any weight \\nprecision between one and nine bits, signed or unsigned, \\na n d  w e i g h t  p r e c i s i o n s  m a y  b e  m i x e d  ( w i t h  s c a l e  normalization) even within a single neuron’s fanout \\ndistribution.', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 4}),\n",
       " Document(page_content='a n d  w e i g h t  p r e c i s i o n s  m a y  b e  m i x e d  ( w i t h  s c a l e  normalization) even within a single neuron’s fanout \\ndistribution. \\n4) Population-based hierarchical connectivity. As a \\ngeneralized weight sharing mechanism, e.g. to support \\nconvolutional neural network types, connectivity \\ntemplates may be defined and mapped to specific population instances during operation. This feature can \\nreduce a network’s required connectivity resources by \\nover an order magnitude. \\nLoihi is the first fully integrated SNN chip that supports any of the \\nabove features. All prior chips , for example the previously mos t \\nsynaptically dense chip [11], store their synapses in dense mat rix \\nform that significantly constrains the space of networks that m ay \\nbe efficiently supported. \\nEach Loihi core includes a programmable learning engine that \\ncan evolve synaptic state variables over time as a function of historical spike activity. In order to support the broadest pos sible', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 4}),\n",
       " Document(page_content='Each Loihi core includes a programmable learning engine that \\ncan evolve synaptic state variables over time as a function of historical spike activity. In order to support the broadest pos sible \\nclass of rules, the learning engine operates on filtered spike traces. \\nLearning rules are microcode programmable and support a rich selection of input terms and output synaptic target variables. \\nSpecific sets of these rules are associated with a learning profile \\nbound to each synapse to be modified. The profile is mapped by some combination of presynaptic neuron, postsynaptic neuron, or  \\nclass of synapse. The learning engine supports simple pairwise \\nSTDP rules and also much more complicated rules such as triplet  \\nSTDP, reinforcement learning with synaptic tag assignments, and  \\ncomplex rules that reference both rate averaged and spike-timin g \\ntraces. \\nAll logic in the chip is digital, functionally deterministic, a nd \\nimplemented in an asynchronous bundled data design style. This', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 4}),\n",
       " Document(page_content='traces. \\nAll logic in the chip is digital, functionally deterministic, a nd \\nimplemented in an asynchronous bundled data design style. This \\nallows spikes to be generated, routed, and consumed in an event -\\ndriven manner with maximal activity gating during idle periods.  \\nThis implementation style is well suited for spiking neural networks that fundamentally feature a high degree of sparseness  \\nin their activity acro ss both space and time. \\n3.2 Mesh Operation \\nFigure 2 shows the operation of the neuromorphic mesh as it \\nexecutes a spiking neural network model. All cores begin at \\nalgorithmic timestep \\nt. Each core independently iterates over its \\nset of neuron compartments, and any neurons that enter a firing \\nstate generate spike messages that the NoC distributes to all c ores     \\n(a) Initial idle state for timestep \\nt. Each square repres ents a core \\nin the mesh containing multiple \\nneurons  (b) Neurons n1 and n2 in cores A and', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 4}),\n",
       " Document(page_content='(a) Initial idle state for timestep \\nt. Each square repres ents a core \\nin the mesh containing multiple \\nneurons  (b) Neurons n1 and n2 in cores A and \\nB fire and generate spike messages  (c) Spikes from all other \\nneurons firing on timestep t in \\ncores A and B ar e  distributed  \\nto their destination cores  (d) Each core advanc es its algorithmic \\ntimestep to t+1 as it handshakes \\nwith its neighbors via barrier \\nsynchronization messages  \\nFig. 2: Mesh Operation  \\nThis article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 4}),\n",
       " Document(page_content='5 \\nthat contain their synaptic fanouts. Spike distributions for tw o \\nsuch example neurons n1 and n2 in cores A and B are illustrated in \\nF i g u r e  2 b ,  w i t h  a d d i t i o n a l  s p i k e  d i s t r i b u t i o n s  f r o m  o t h e r  f i r i n g \\nneurons adding to the NoC traffic in Figure 2c. \\nThe NoC distributes spike (and all other) messages according \\nto a dimension-order routing algorithm. The NoC itself only \\nsupports unicast distributions. To multicast spikes, the output  \\nprocess of each core iterates over a list of destination cores for a \\nfiring neuron’s fanout distribution and sends one spike per cor e. \\nFor deadlock protection reasons relating to read and chip-to-ch ip \\nm e s s a g e  t r a n s a c t i o n s ,  t h e  m e s h  u s e s  t w o  i n d e p e n d e n t  p h y s i c a l  r o u t e r  n e t w o r k s .  F o r  b a n d w i d t h  e f f i c i e n c y ,  t h e  c o r e s  a l t e r n a t e  \\nsending their spike messages across the two physical networks.', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 5}),\n",
       " Document(page_content='sending their spike messages across the two physical networks. \\nThis is possible because SNN computation does not depend on the  \\nspike sequence ordering within a timestep. \\nAt the end of the timestep, a mechanism is needed to ensure \\nthat all spikes have been delivered and that it’s safe for the cores \\nproceed to timestep \\nt + 1. Rather than using a globally distributed \\nt i m e  r e f e r e n c e  ( c l o c k )  t h a t  m u s t  p e s s i m i z e  f o r  t h e  w o r s t - c a s e  chip-wide network activity, we use a barrier synchronization \\nmechanism, illustrated in Figure 2d. As each core finishes serv icing \\nits compartments for timestep \\nt, it exchanges barrier messages \\nwith its neighboring cores. The barrier messages flush any spik es \\nin flight and, in a second phase, propagate a timestep-advance notification to all cores. As cores receive the second phase of  \\nbarrier messages, they advance their timestep and proceed to \\nupdate compartments for time \\nt + 1.', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 5}),\n",
       " Document(page_content='barrier messages, they advance their timestep and proceed to \\nupdate compartments for time \\nt + 1. \\nAs long as management activity is restricted to a specific \\n“preemption” phase of the barrier synchronization process that any embedded x86 core or off-chip host may introduce on \\ndemand, the Loihi mesh is provably deadlock free. \\n3.3 Network Connectivity Architecture \\nIn its most abstract formulation, the neural network mapped to \\nthe Loihi architecture is a directed multigraph structure \\n࣡N ,S), \\nwhere N i s  t h e  s e t  o f  n e u r o n s  i n  t h e  n e t w o r k  a n d  S i s  a  s e t  o f  \\nsynapses (edges) connecting pa irs of neurons. Each synapse s ∈ S \\ncorresponds to a 5-tuple: (i,j,wgt,dly,tag), where i,j ∈ N identify \\nthe source and destination neurons of the synapse, and wgt, dly, \\nand tag are integer-valued properties of the synapse. In general, \\nLoihi will autonomously modify the synaptic variables \\n(wgt,dly,tag) according to programmed learning rules. All other', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 5}),\n",
       " Document(page_content='and tag are integer-valued properties of the synapse. In general, \\nLoihi will autonomously modify the synaptic variables \\n(wgt,dly,tag) according to programmed learning rules. All other \\nnetwork parameters remain constant unless they are modified by x86 core intervention. \\nAn abstract network is mapped to the mesh by assigning \\nneurons to cores, subject to each core’s resource constraints. \\nFigure 3 shows an example of a simple seven neuron network mapped to three cores. Given a particular neuron-to-core \\nmapping for \\nN, each neuron’s synaptic fanin state ( wgt, dly, and \\ntag) must be stored in the core’s synaptic memory. These \\nschematically correspond to the synaptic spike markers in Figur e \\n3. Each neuron’s fanout edges are projected to a list of core-t o-\\ncore edges (colored yellow), and each core-to-core edge is \\nassigned an axon_id identifier unique to each destination core \\n(colored red). The neuron’s synaptic fanout contained within ea ch', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 5}),\n",
       " Document(page_content='core edges (colored yellow), and each core-to-core edge is \\nassigned an axon_id identifier unique to each destination core \\n(colored red). The neuron’s synaptic fanout contained within ea ch \\ndestination core is associated with the corresponding axon_id a nd organized as a list of 4-tuples (j,wgt,dly,tag) stored in the synaptic \\nmemory in some suitably compressed form.  When neuron i \\nspikes, the mesh routes each axon_id to the appropriate fanout \\ncore which then expands it to the corresponding synaptic list. \\nThis connectivity architecture can support arbitrary \\nmultigraph networks subject to t he cores’ resource constraints:  \\n1) The total number of neurons assigned to any core may \\nnot exceed 1,024 ( Ncx). \\n2) The total synaptic fanin state mapped to any core must \\nnot exceed 128KB ( Nsyn × 64 b, subject to compression \\nand list alignment considerations.) \\n3) The total number of core-to-core fanout edges mapped \\nto any given core must not exceed 4,096 ( Naxout). This', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 5}),\n",
       " Document(page_content='and list alignment considerations.) \\n3) The total number of core-to-core fanout edges mapped \\nto any given core must not exceed 4,096 ( Naxout). This \\ncorresponds to the number of output-side routing slots \\nhighlighted in yellow in Figure 3. \\n4) The total number of distribution lists, associated by \\naxon_id, in any core must not exceed 4,096 ( Naxin). This \\nis the number of input-side axon_id routing slots \\nhighlighted in red in Figure 3. \\nIn practice, constraints 2 and 4 tend to be the most limiting. \\nIn order to exploit structure that may exist in the network ࣡ ,\\nLoihi supports a hierarchical network model. This feature can \\nsignificantly reduce the chip-wide connectivity and synaptic \\nresources needed to map convolutional-style networks in which a  \\ntemplate of synaptic connections is applied to many neurons in a \\nuniform way. \\nFormally, we represent the hierarchical template network as a \\ndirected multigraph ℋ = (࣮,ℰ  )where ࣮ is a set of disjoint neuron', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 5}),\n",
       " Document(page_content='uniform way. \\nFormally, we represent the hierarchical template network as a \\ndirected multigraph ℋ = (࣮,ℰ  )where ࣮ is a set of disjoint neuron \\npopulation types and ℰ defines a set of edges connecting between \\npairs Tsrc,Tdst ∈ ࣮ An edge E ∈ ℰ associated with the (Tsrc,Tdst) \\npopulation type pair is a set of synapses where each s ∈ E \\nconnects a neuron i ∈ Tsrc to to a neuron j ∈ Tdst. \\nIn order to hierarchically compress the resource mapping of \\nthe desired flat network ࣡N ,S), a set of disjoint neuron \\npopulations instances ࣪ must be defined where each P ∈ ࣪ is a \\nsubset of neurons P ⊂ N. Each population instance is associated \\nwith a population type T ∈ ࣮ from the hierarchical template F\\nE\\nDCORE3\\nC\\nB\\nCORE2X\\nA\\nCORE1\\naxon_id 1axon_id 2axon_id 3\\nF\\nE\\nD\\nC\\nB\\nAX\\nNaxin Naxout\\nFig. 3: Neuron-to-neuron mesh routing model  This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 5}),\n",
       " Document(page_content='Fig. 3: Neuron-to-neuron mesh routing model  This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 5}),\n",
       " Document(page_content='6 \\nnetwork ℋ. Neurons n ∈ N  belonging to some population \\ninstance P ∈ ࣪ are said to be population-mapped . By configuring \\nthe ℋ connectivity in hardware, the redundant connectivity in ࣡ \\nis implied and doesn’t consume resources, beyond what it takes \\nto map the population-level connectivity of ℋ as if it were a flat \\nnetwork. \\nPopulation-mapped neurons produce population spike \\nmessages whose axon_id fields identify (1) the destination \\npopulation Pdst, (2) the source neuron index i ∈ Psrc within the \\ns o u r c e  p o p u l a t i o n ,  a n d  ( 3 )  t h e  p a r t i c u l a r  e d g e  c o n n e c t i n g  \\nbetween Tsrc and Tdst w h e n  t h e r e  i s  m o r e  t h a n  o n e .  O n e  \\npopulation spike must be sent per destination population rather  \\nthan per destination core, as in the flat case. This marginally  higher \\nlevel of spike traffic is more than offset by the savings in ne twork \\nmapping resources. \\nConvolutional artificial neural networks (ConvNets), in which a', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 6}),\n",
       " Document(page_content='level of spike traffic is more than offset by the savings in ne twork \\nmapping resources. \\nConvolutional artificial neural networks (ConvNets), in which a  \\nsingle kernel of weights is repeatedly applied to different pat ches \\nof input pixels, is an example class of network that greatly be nefits \\nfrom hierarchy. By treating such a weight kernel as the templat e \\nconnectivity that is applied to the different image patches \\n(population instances), Loihi can support a spiking form of suc h \\nnetworks. The S-LCA network discussed in Section 5.2 features a  \\nsimilar kernel-style convolutional network topology which \\nadditionally includes lateral inhibitory connections between th e \\nfeature neurons of each population instance. \\n3.4 Learning Engine \\n3.4.1 Baseline STDP \\nA number of neuromorphic chip architectures to date have \\nincorporated the most basic form of pairwise, nearest-neighbor \\nspike time dependent plasticity (STDP). Pairwise STDP is simple ,', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 6}),\n",
       " Document(page_content='A number of neuromorphic chip architectures to date have \\nincorporated the most basic form of pairwise, nearest-neighbor \\nspike time dependent plasticity (STDP). Pairwise STDP is simple , \\nevent-driven, and highly amenable to hardware implementation. \\nFor a given synapse connecting presynaptic neuron j to \\npostsynaptic neuron i, an implementation needs only maintain \\nthe most recent spike times for the two neurons (\\u0bdd\\u0be3\\u0be5\\u0bd8 and \\u0bdc\\u0be3\\u0be2௦௧). \\nGiven a spike arrival at time t, one local nonlinear computation \\nneeds to be evaluated in order to update the synaptic weight: \\n\\u0bdc\\u0be3\\u0be2௦௧൯, On presynaptic spike\\n\\u0bdd\\u0be3\\u0be5\\u0bd8൯, On postsynaptic spike \\n( 3 ) \\nwhere )ݔis some approximation of )ݔ(ܪ for constants \\nA− < 0, A+ > 0, and τ > 0. Since a design must already perform a \\nlookup of weight wi,j on any presynaptic spike arrival, the first case \\nabove matches the natural dataflow present in any neuromorphic \\nimplementation. To support this depressive half of the STDP', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 6}),\n",
       " Document(page_content='lookup of weight wi,j on any presynaptic spike arrival, the first case \\nabove matches the natural dataflow present in any neuromorphic \\nimplementation. To support this depressive half of the STDP \\nlearning rule, the handling of a presynaptic spike arrival simp ly \\nt u r n s  a  r e a d  o f  t h e  w e i g h t  s t a t e  i n t o  a  r e a d - m o d i f y - w r i t e  \\noperation, assuming availability of the tpost spike time. \\nThe potentiating half of Equation 3 is the only significant \\nchallenge that pairwise STDP introduces. To handle this weight \\nupdate in an event-driven manner, symmetric to the depressive \\ncase, the implementation needs to perform a backwards routing \\ntable lookup, obtaining wi,j from the firing postsynaptic neuron i. \\nThis is at odds with the algorithmic impetus for more complex a nd diverse network routing functions R : j → Y , where i ∈ Y . The \\nmore complex R b e c o m e s ,  t h e  m o r e  e x p e n s i v e ,  i n  g e n e r a l ,  i t', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 6}),\n",
       " Document(page_content='more complex R b e c o m e s ,  t h e  m o r e  e x p e n s i v e ,  i n  g e n e r a l ,  i t  \\nbecomes to implement an inverse lookup R−1 efficiently in \\nhardware. Some implementations have explored creative \\nsolutions to this problem [12], but in general these approaches  \\nconstrain network topologies and are not scalable. \\nFor Loihi, we adopt a less event-driven epoch-based synaptic \\nmodification architecture in the interest of supporting arbitra rily \\ncomplex R and extending the architecture to more advanced \\nlearning rules. This architecture delays the updating of all sy naptic \\nstate to the end of a periodic learning epoch time Tepoch. \\nAn epoch-based architecture fundamentally requires iteration \\nover each core’s active input axons, which Loihi does sequentia lly. \\nIn theory this is a disadvantage that a direct implementation o f the \\nR−1 reverse lookup may avoid. However, in practice, any pipelined \\ndigital core implementation stil l requires iteration over activ e', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 6}),\n",
       " Document(page_content='R−1 reverse lookup may avoid. However, in practice, any pipelined \\ndigital core implementation stil l requires iteration over activ e \\ninput axons in order to maintain spike timestamp or trace state . \\nEven the fully transposable synaptic crossbar architecture used  in \\n[12] includes an iteration over all input axons per timestep fo r this \\nreason. \\n3.4.2 Advancing Beyond Pairwise STDP \\nA number of architectural challenges arise in the pursuit of \\nsupporting more advanced learning rules. First, the functional \\nforms describing ∆wi,j become more complex and seemingly \\narbitrary. These rules are at the frontier of algorithm researc h and \\ntherefore require a high degree of configurability. Second, the  \\nrules involve multiple synaptic variables, not just weights. Fi nally, \\nadvanced learning rules rely on temporal correlations in spikin g \\nactivity over a range of timescales, which means more than just  \\nthe most recent spike times must be maintained. These challenge s', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 6}),\n",
       " Document(page_content='advanced learning rules rely on temporal correlations in spikin g \\nactivity over a range of timescales, which means more than just  \\nthe most recent spike times must be maintained. These challenge s \\nmotivate the central features of Loihi’s learning architecture,  \\ndescribed below. \\n3.4.3 Learning Rule Functional Form \\nOn every learning epoch, a synapse will be updated whenever the  \\nappropriate pre- or post-synaptic conditions are satisfied. A s et of \\nmicrocode operations associated with the synapse determines the  \\nfunctional form of one or more transformations to apply to the synapse’s state variables. The rules are specified in sum-of-\\nproducts form: \\n\\u0bdc,\\u0bdd)\\u0be1\\u0cd4\\n\\u0bddୀଵேು\\n\\u0bdcୀଵ \\n \\n( 4 ) \\nwhere z is the transformed synaptic variable (either wgt, dly, or \\ntag), Vi,j refers to some choice of input variable available to the \\nlearning engine, and Ci,j and Si are microcode-specified signed \\nconstants. \\nTable 1 provides a comprehensive list of product terms as', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 6}),\n",
       " Document(page_content='learning engine, and Ci,j and Si are microcode-specified signed \\nconstants. \\nTable 1 provides a comprehensive list of product terms as \\nencoded by a 4-bit field in each microcode op. The multiplicati ons \\nand summations of Equation 4 are computed iteratively by the hardware and accumulated in 16-bit registers. The epoch period \\nis globally configured per core u p to a maximum value of 63, wi th \\nTi,j\\nPiThis article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 6}),\n",
       " Document(page_content='7 \\ntypical values in the 2 to 8 range. To avoid receiving more tha n \\none spike in a given epoch, the epoch period is normally set to  the \\nminimum refractory delay of all neurons in the network. \\n \\nThe basic pairwise STDP rule only requires two products \\ninvolving four of these terms (0, 1, 3, and 4) and two constant s. \\nThe Loihi microcode format can specify this rule in a single 32 -bit \\nword. With an encoding capacity of up to sixteen 32-bit words a nd \\nthe full range of terms in Table 1, the learning engine provide s \\nconsiderable headroom for far more complex rules. \\n3.4.4 Trace Evaluation \\nThe trace variables ( x1,x2,y1,y2,y3,r1)  i n  T a b l e  1  r e f e r  t o  f i l t e r e d  \\nspike trains associated with each synapse that the learning eng ine \\nmodifies. The filtering function associated with each trace is \\ndefined by two configurable quantities: an impulse amount δ \\nadded on every spike event and a decay factor α. Given a spike', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 7}),\n",
       " Document(page_content='modifies. The filtering function associated with each trace is \\ndefined by two configurable quantities: an impulse amount δ \\nadded on every spike event and a decay factor α. Given a spike \\narrival sequence s[t] ∈ {0,1}, an ideal trace sequence x[t] over \\ntime is defined as follows: \\n]. \\n( 5 ) \\nThe Loihi hardware computes a low-precision (seven bit) \\napproximation of this first-order  filter using stochastic round ing. \\nBy setting δ to 1 (typically with relatively small α), x[t] \\nsaturates on each spike and its decay measures elapsed time sin ce \\nthe most recent spike. Such trace configurations exactly implement the baseline STDP rules dependent only on nearest-\\nneighbor pre/post spike time separations described in Section \\n3.4.1. On the other hand, setting \\nδ t o  a  v a l u e  l e s s  t h a n  1 ,  \\nspecifically 1 − αTmin , where Tmin is the minimum spike period, \\ncauses sufficiently closely spaced spike impulses to accumulate', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 7}),\n",
       " Document(page_content='δ t o  a  v a l u e  l e s s  t h a n  1 ,  \\nspecifically 1 − αTmin , where Tmin is the minimum spike period, \\ncauses sufficiently closely spaced spike impulses to accumulate  \\nover time and x[t] reflects the average spike rate over a timescale \\nof τ = −1 /log α. 4 D ESIGN IMPLEMENTATION \\n4.1 Core Microarchitecture \\nFigure 4 shows the internal structure of the Loihi neuromorphic  \\ncore. Colored blocks in this diagram represent the major \\nmemories that store the connectivity, configuration, and dynami c \\nstate of all neurons mapped to the core. The core’s total SRAM capacity is 2Mb including ECC overhead. The coloring of memorie s \\nand dataflow arcs illustrates th e core’s four primary operating  \\nmodes: input spike handling (green), neuron compartment updates (purple), output spike generation (blue), and synaptic \\nupdates (red). Each of these modes operates independently with \\nminimal synchronization at a variety of frequencies, based on t he', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 7}),\n",
       " Document(page_content='updates (red). Each of these modes operates independently with \\nminimal synchronization at a variety of frequencies, based on t he \\nstate and configuration of the core. The black structure marked  \\nUCODE represents the configurable learning engine. \\nThe values annotated by each memory indicate its number of \\nlogical addresses, which correspond to the core’s major resourc e \\nconstraints. The number of input and output axons (\\nNaxin and \\nNaxout), the synaptic memory size ( Nsyn), and the total number of \\nneuron compartments ( Ncx) impose network connectivity \\nconstraints as described in Section 3.3. The parameter Nsdelay \\nindicates the minimum number of synaptic delay units supported,  \\neight in Loihi. Larger synaptic delay values, up to 62, may be supported when fewer neuron compartments are needed by a \\nparticular mapped network. \\nVarying degrees of parallelism and serialization are applied to  \\nsections of the core’s pipeline in order to balance the through put', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 7}),\n",
       " Document(page_content='particular mapped network. \\nVarying degrees of parallelism and serialization are applied to  \\nsections of the core’s pipeline in order to balance the through put \\nbottlenecks that typical workloads will encounter. Dataflow drawn with finely dotted arrows in Figure 4 indicate parts of t he \\ndesign where single events are expanded into a potentially larg e \\nnumber of dependent events. In these areas, we generally parallelize the hardware. \\nFor example, synapses are extracted from SYNAPSE_MEM’s \\n64-bit words with up to four-way parallelism, depending on the \\nsynaptic encoding format, and that parallelism is extended to \\nDENDRITE_ACCUM and throughout the synaptic modification pipeline in the learning engine. Conversely, the presynaptic tr ace \\nstate is stored together with SY NAPSE_MEM pointer entries in th e \\nSYNAPSE_MAP memory, which then may result in multiple serial accesses per ingress spike. This balances pipeline throughputs for', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 7}),\n",
       " Document(page_content='state is stored together with SY NAPSE_MEM pointer entries in th e \\nSYNAPSE_MAP memory, which then may result in multiple serial accesses per ingress spike. This balances pipeline throughputs for \\ningress learning-enabled axons when their synaptic fanout facto r \\nwithin the core is on the order of 10:1 while maintaining the b est \\npossible area efficiency. \\nRead-modify-write (RMW) memory accesses, shown as loops \\naround the relevant memories in Figure 4, are fundamental to th e \\nneuromorphic computational model and unusually pervasive \\ncompared to many other microarchitecture domains. Such loops \\ncan introduce significant design challenges, particularly for \\nperformance. We manage this challenge with an asynchronous design pattern that encapsulates and distributes the memory’s \\nstate over a collection of single-ported SRAM banks. The \\nencapsulation wrapper presents a simple dual-ported interface t o \\nthe environment logic and avoids severely stalling the pipeline', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 7}),\n",
       " Document(page_content='state over a collection of single-ported SRAM banks. The \\nencapsulation wrapper presents a simple dual-ported interface t o \\nthe environment logic and avoids severely stalling the pipeline  \\nexcept for statistically rare address conflicts. Encoding Term (Ti,j) Bits Description \\n0 x0 +C 5b (U)  Presynaptic spike count  \\n1 x1 +C 7b (U)  1st presynaptic trace  \\n2 x2 +C 7b (U)  2nd presynaptic trace  \\n3 y0 +C 5b (U)  Postsynaptic spike count  \\n4 y1 +C 7b (U)  1st postsynaptic trace  \\n5 y2 +C 7b (U)  2nd postsynaptic trace  \\n6 y3 +C 7b (U)  3rd postsynaptic trace  \\n7 r0 +C 1b (U)  Reward spike  \\n8 r1 +C 8b (S)  Reward trace  \\n9 wgt+C 9b (S)  Synaptic weight  \\n10 dly+C 6b (U)  Synaptic delay  \\n11 tag+C 9b (S)  Synaptic tag  \\n12 sgn(wgt+C) 1b (S)  Sign of case 9 ( ±1) \\n13 sgn(dly+C) 1b (S)  Sign of case 10 ( ±1) \\n14 sgn(tag+C) 1b (S)  Sign of case 11 ( ±1) \\n15 C 8b (S)  Constant term. (Variant 1)  \\n15 Sm · 2Se 4b (S)  Scaling term. 4b mantissa, \\n4b exponent. (Variant 2)', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 7}),\n",
       " Document(page_content='14 sgn(tag+C) 1b (S)  Sign of case 11 ( ±1) \\n15 C 8b (S)  Constant term. (Variant 1)  \\n15 Sm · 2Se 4b (S)  Scaling term. 4b mantissa, \\n4b exponent. (Variant 2)  \\nTABLE 1: Learning rule product terms This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 7}),\n",
       " Document(page_content='8 \\n4.2 Asynchronous Design Methodology \\nBiological neural networks are fundamentally asynchronous, as \\nreflected by the absence of an explicit synchronization assumpt ion \\nin the continuous time SNN model given in Section 2. Accordingl y, \\nasynchronous design methods have long been seen as the appropriate tool for prototyping spiking neural networks in sil icon, \\nand most published chips to date use this methodology. Loihi is  no \\ndifferent and in fact the asynchronous design methodology developed for Loihi is the most advanced of its kind. \\nFor rapid neuromorphic design prototyping, we extended and \\nimproved on an earlier asynchronous design methodology used to \\ndevelop several generations of c ommercial Ethernet switches. In  \\nthis methodology, designs are entered according to a top-down decomposition process using the CAST and CSP languages. \\nModules in each level of design hierarchy communicate over \\nmessage-passing channels that ar e later mapped to a circuit-lev el', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 8}),\n",
       " Document(page_content='Modules in each level of design hierarchy communicate over \\nmessage-passing channels that ar e later mapped to a circuit-lev el \\nimplementation, which in this case is a bundled data \\nimplementation comprising a data payload with request and \\nacknowledge handshaking signals that mediate the propagation of  \\ndata tokens through the system. Figure 5 shows a template \\np i p e l i n e  e x a m p l e .  E a c h  p i p e l i n e  s t a g e  h a s  a t  l e a s t  o n e  p u l s e  \\ngenerator, such as the one shown in Figure 6, that implements t he \\ntwo-phase handshake and latch sequencing. \\nFine-grain flow control is an important property of \\nasynchronous design that offers several benefits for \\nneuromorphic applications. First, since the activity in SNNs is  \\nhighly sparse in both space and time, the activity gating that \\ncomes automatically with asynchronous flow control eliminates \\nthe power that would often be wasted by a continuously running', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 8}),\n",
       " Document(page_content='highly sparse in both space and time, the activity gating that \\ncomes automatically with asynchronous flow control eliminates \\nthe power that would often be wasted by a continuously running \\nclock. Second, local flow control allows different modules in t he \\ns a m e  d e s i g n  t o  r u n  a t  t h e i r  n a t u r a l  m i c r o a r c h i t e c t u r a l  \\nfrequencies. This properly complements the need for spiking \\nneuron processes to run at a variety of timescales dependent on  \\nworkload and can significantly simplify back-end timing closure . \\nFinally, asynchronous techniques can reduce or eliminate timing  \\nmargin. In Loihi, the mesh-level barrier synchronization mechanism is the best example of asynchronous handshaking providing a globally significant performance advantage by \\neliminating needless mesh-wide idle time. \\nGiven a hierarchical design decomposition written in CSP, a \\npipeline synthesis tool converts the CSP module descriptions to', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 8}),\n",
       " Document(page_content='eliminating needless mesh-wide idle time. \\nGiven a hierarchical design decomposition written in CSP, a \\npipeline synthesis tool converts the CSP module descriptions to  \\nV e r i l o g  r e p r e s e n t a t i o n s  t h a t  a r e  c o m p a t i b l e  w i t h  s t a n d a r d  E D A  \\ntools. The initial Verilog representation supports logic synthe sis to \\nboth synchronous and asynchronous implementations with full functional equivalence, providing support for synchronous FPGA \\nemulation of the design. \\nFig. 4: Core Top-Level Microarchitecture. The SYNAPSE unit proc esses all incoming spikes and reads out the associated synaptic  weights from the memory. \\nThe DENDRITE unit updates the st ate variables u and v of all ne urons in the core. The AXON unit generates spike messages for a ll fanout cores of each firing \\nneuron. The LEARNING unit update s synaptic weights using the pr ogrammed learning rule s at epoch boundaries.  \\nL \\nA \\nT \\nC \\nH \\nPULSE \\nGENERATOR  \\nEN \\n DEN \\nL \\n. \\nq \\n R \\n.', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 8}),\n",
       " Document(page_content='neuron. The LEARNING unit update s synaptic weights using the pr ogrammed learning rule s at epoch boundaries.  \\nL \\nA \\nT \\nC \\nH \\nPULSE \\nGENERATOR  \\nEN \\n DEN \\nL \\n. \\nq \\n R \\n. \\nq \\nR \\n. \\na \\n L \\n. \\na \\nPulse \\n- \\nwidth  \\nextension  \\nPipeline  \\n p\\ndatapath  \\nDATA \\nIN \\nDATA \\nOUT \\nL \\nA \\nT \\nC \\nH \\nL \\nA \\nT \\nC \\nH \\nLogic \\nFig. 5: Bundled data pipeline stage \\nEN \\n DEN \\nR \\n. \\nq \\nR \\n. \\na \\nL \\nL\\n. \\na \\nL \\nL\\n. \\nq \\n LATCH \\n D \\n Q \\nPULSE \\nGENERATOR  \\nFig. 6: Bundled data pulse generator circuit This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 8}),\n",
       " Document(page_content='9 \\nThe asynchronous back-end layout flow uses standard tools \\nwith an almost fully standard cell library. Here, the asynchron ous \\nmethodology simplifies the layout closure problem. At every lev el \\nof layout hierarchy, all timing constraints apply only to neighboring, physically proximate pipeline stages. This greatly  \\nfacilitates convergent timing closure, especially at the chip l evel. \\nFor example, the Loihi mesh assembles by physical abutment \\nwithout needing any unique clock distribution layout or timing \\nanalysis for different mesh dimensions or core types. \\n5 R ESULTS \\n5.1 Silicon Realization \\nLoihi was fabbed in Intel’s 14nm FinFET process. The chip \\ninstantiates a total of 2.07 billion transistors and 33 MB of S RAM \\nover its 128 neuromorphic cores an d three x86 cores, with a die  \\narea of 60 mm2. The device is functional over a supply voltage \\nrange of 0.50V to 1.25V. Table 2 provides a selection of energy  and \\nperformance measurements from pre-silicon SDF and SPICE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 9}),\n",
       " Document(page_content='area of 60 mm2. The device is functional over a supply voltage \\nrange of 0.50V to 1.25V. Table 2 provides a selection of energy  and \\nperformance measurements from pre-silicon SDF and SPICE \\nsimulations, consistent with early post-silicon characterizatio n. \\nLoihi includes a total of 16MB of synaptic memory. With its \\ndensest 1-bit synapse format, this provides a total of 2.1 mill ion \\nunique synaptic variables per mm2, over three times higher than \\nTrueNorth, the previously most dense SNN chip [11]. This does n ot \\nconsider Loihi’s hierarchical network support that can signific antly \\nb o o s t  i t s  e f f e c t i v e  s y n a p t i c  d e n s i t y .  O n  t h e  o t h e r  h a n d ,  L o i h i ’ s \\nmaximum neuron density of 2,184 per mm2 is marginally worse \\nthan TrueNorth’s. Process normalized, this represents a 2× \\nreduction in the design’s neuron density, which may be \\ninterpreted as the cost of Loihi’s greatly expanded feature set , an \\nintentional design choice.', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 9}),\n",
       " Document(page_content='reduction in the design’s neuron density, which may be \\ninterpreted as the cost of Loihi’s greatly expanded feature set , an \\nintentional design choice. \\n5.2 Algorithmic Results \\nOn an earlier iteration of the Loihi architecture, we quantitat ively \\nassessed the efficiency of Spiking LCA to solve LASSO, as descr ibed \\nin Section 2.2. We used a 1.67 GHz Atom CPU running both LARS and FISTA [3] numerical solvers as a reference architecture for  \\nbenchmarking. These solvers are among the best known for this \\nproblem. Both chips were fabbed in 14nm technology, were \\nevaluated at a 0.75V supply voltage, and required similar activ e \\nsilicon areas (5 mm2). \\nMeasured parameter Value at 0.75V \\nCross-sectional spike bandwidth per tile  3.44 Gspike/s  \\nWithin-tile spike energy  1.7 pJ  \\nWithin-tile spike latency  2.1 ns  \\nEnergy per tile hop (E-W / N-S)  3.0 pJ / 4.0 pJ  \\nLatency per tile hop (E-W / N-S)  4.1 ns / 6.5 ns  \\nEnergy per synaptic spike op (min)  23.6 pJ', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 9}),\n",
       " Document(page_content='Within-tile spike latency  2.1 ns  \\nEnergy per tile hop (E-W / N-S)  3.0 pJ / 4.0 pJ  \\nLatency per tile hop (E-W / N-S)  4.1 ns / 6.5 ns  \\nEnergy per synaptic spike op (min)  23.6 pJ  \\nTime per synaptic spike op (max)  3.5 ns  \\nEnergy per synaptic update (pairwise STDP)  120 pJ  \\nTime per synaptic update (pairwise STDP)  6.1 ns  \\nEnergy per neuron update (active / inactive)  81 pJ / 52 pJ  \\nTime per neuron update (active / inactive)  8.4 ns / 5.3 ns  \\nMesh-wide barrier sync time (1-32 tiles)  113-465ns  \\nTABLE 2: Loihi pre-silicon performance and energy measurements \\n \\n \\n(a) Original  \\n(b) Reconstruction \\nFig. 8: Image reconstruction f rom the sparse coefficients \\ncomputed using the Loihi predecessor.  \\nThe largest problem we evaluated is a convolutional sparse \\ncoding problem on a 52 ×52 image with a 224-atom dictionary, a \\npatch size of 8 ×8, and a patch stride of 4 pixels. Loihi’s hierarchical \\nconnectivity provided a factor of 18 compression in synaptic', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 9}),\n",
       " Document(page_content='coding problem on a 52 ×52 image with a 224-atom dictionary, a \\npatch size of 8 ×8, and a patch stride of 4 pixels. Loihi’s hierarchical \\nconnectivity provided a factor of 18 compression in synaptic \\nresources for this network. We solved the sparse coding problem  \\nto a solution within 1% of the optimal solution. Figure 8 compa res \\nt h e  o r i g i n a l  a n d  t h e  r e c o n s t r u c t e d  i m a g e  u s i n g  t h e  c o m p u t e d  \\nsparse coefficients. \\nTable 3 shows the comparison in computational efficiency \\nbetween these two architectures, as measured by EDP. It is not surprising to see that the conventional LARS solver can handle \\nproblems of small sizes and very sparse solutions quite efficie ntly. \\nOn the other hand, the conventional solvers do not scale well f or \\nthe large problem and the Loihi predecessor achieves the target  \\nobjective value with over 5,000 times lower EDP. \\n \\n \\n \\n \\n  \\n \\nNo. Unknowns 400 1,700 32,256 \\nNo. nonzeros in solutions  ≈10 ≈30 ≈420', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 9}),\n",
       " Document(page_content='objective value with over 5,000 times lower EDP. \\n \\n \\n \\n \\n  \\n \\nNo. Unknowns 400 1,700 32,256 \\nNo. nonzeros in solutions  ≈10 ≈30 ≈420 \\nEnergy  2.58x 8.08x 48.74x  \\nDelay 0.27x 2.76x 118.18x  \\nEDP 0.7x 22.33x  5760x  \\nTABLE 3: Comparison of solving ℓ1 minimization on Loihi and Atom. \\nResults are expressed as improvement ratios Atom/Loihi. The Ato m \\nnumbers are chosen from using th e more efficient solver between  LARS \\nand FISTA. \\nFig. 7: Loihi chip plot This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 9}),\n",
       " Document(page_content='10 \\nLoihi’s flexible learning engine allows one to explore and \\nexperiment with various learning methods. We have developed \\na n d  v a l i d a t e d  t h e  f o l l o w i n g  n e t w o r k s  i n  p r e - s i l i c o n  F P G A  \\nemulation with all learning taking place on chip: \\n• A single-layer classifier using a supervised variant of STDP \\nsimilar to [4] as the learning method. This network, when \\ntrained with local-intensity-change based temporally spike-coded image samples, can achieve 96% accuracy on \\nthe MNIST dataset using ten neurons, in line with a \\nreference ANN of the same structure. \\n• Solving the shortest path problem of a weighted graph. \\nVertices and edges are represented as neurons and \\nsynapses respectively. The algorithm is based on the \\neffects of STDP on a propagating wavefront of spikes [13]. \\n• Solving a one-dimensional, non-Markovian sequential \\ndecision making problem. The network learns the decision making policy in response to delayed reward and', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 10}),\n",
       " Document(page_content='• Solving a one-dimensional, non-Markovian sequential \\ndecision making problem. The network learns the decision making policy in response to delayed reward and \\npunishment feedback similar to [14]. \\nThe algorithmic development and characterization of Loihi is \\njust beginning. These proof-of-concept examples use only a fraction of the resources and features available in the chip. W ith \\nLoihi now in hand, our focus turns to scaling and further evalu ating \\nthese networks. \\n6 C ONCLUSION \\nLoihi is Intel’s fifth and most complex fabricated chip in a fa mily of \\ndevices that explore different points in the neuromorphic desig n \\nspace spanning architectural variations, circuit methodologies,  \\nand process technology. In some respects, its flexibility may g o too \\nfar, while in others, not far enough. Further optimizations of the \\narchitecture and implementation are planned. The pursuit of \\ncommercially viable neuromorphic architectures and algorithms', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 10}),\n",
       " Document(page_content='far, while in others, not far enough. Further optimizations of the \\narchitecture and implementation are planned. The pursuit of \\ncommercially viable neuromorphic architectures and algorithms \\nmay well end at design points far from what we have described i n \\nthis paper, but we hope Loihi provides a step in the right dire ction. \\nWe offer it as a vehicle for coll aborative exploration with the  \\nbroader research community. \\nREFERENCES \\n[1] P. T. P. Tang, T.-H. Lin, and M. Davies, “Sparse coding by spik ing neural \\nnetworks: Convergence theory and computational results,” arXiv e-\\nprints , 2017.  \\n[2] S. Shapero, M. Zhu, J. Hasler, and C. Rozell, “Optimal sparse \\napproximation with integrate and fire neurons,” International journal of \\nneural systems , vol. 24, no. 5, p. 1440001, 2014.  \\n[3] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholdi ng \\nalgorithm for linear inverse problems,” SIAM Journal on Imaging \\nSciences , vol. 2, no. 1, pp. 183–202, 2009.', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 10}),\n",
       " Document(page_content='[3] A. Beck and M. Teboulle, “A fast iterative shrinkage-thresholdi ng \\nalgorithm for linear inverse problems,” SIAM Journal on Imaging \\nSciences , vol. 2, no. 1, pp. 183–202, 2009.  \\n[4] F. Ponulak and A. Kasinski, “Supervised learning in spiking neu ral´ \\nnetworks with ReSuMe: sequence learning, classification, and sp ike \\nshifting,” Neural Computation , vol. 22, no. 2, pp. 467–510, 2010.  \\n[5] T.-H. Lin, “Local Information wi th Feedback Perturbation Suffic es for \\nDictionary Learning in  Neural Circuits,” arXiv e-prints , 2017.  \\n[6] E. Neftci, C. Augustine, S. Paul, and G. Detorakis, “Event-Driv en Random \\nBack-Propagation: Enabling Neuro morphic Deep Lea rning Machines, ” \\nFrontiers in neuroscience , vol. 11, p. 324, 2017.  \\n[7] J. Gjorgjieva, C. Clopath, J. Audet, and J.-P. Pfister, “A trip let spike-\\ntiming dependent plasticity model generalizes the Bienenstock C ooper \\nMunro rule to higher-order s patiotemporal correlations,” Proceedings', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 10}),\n",
       " Document(page_content='timing dependent plasticity model generalizes the Bienenstock C ooper \\nMunro rule to higher-order s patiotemporal correlations,” Proceedings \\nof the National Academy of Sciences , vol. 108, no. 48, pp. 19 383–19 \\n388, 2011.  [8] N. Qiao, H. Mostafa, F. Corradi, M. Osswald, F. Stefanini, D. \\nSumislawska, and G. Indiveri, “A reconfigurable on-line learnin g spiking \\nneuromorphic processor comprising 256 neurons and 128K synapses ,” \\nFrontiers in Neuroscience , vol. 9, p. 141, 2015.  \\n[9] L. Buesing, J. Bill, B. Nessler, and W. Maass, “Neural dynamics  as \\nsampling: a model for stochastic computation in recurrent netwo rks of \\nspiking neurons,” PLoS computational biology , vol. 7, no. 11, p. \\ne1002211, 2011.  \\n[10] E. M. Izhikevich, “Polychronizat ion: Computation with Spikes,” Neural \\nComputation , vol. 18, no. 2, pp. 245–282, 2006, pMID: 16378515.  \\n[11] P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy,', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 10}),\n",
       " Document(page_content='Computation , vol. 18, no. 2, pp. 245–282, 2006, pMID: 16378515.  \\n[11] P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy,  \\nJ. Sawada, F. Akopyan, B. L. Jack son, N. Imam, C. Guo, Y. Nakam ura, B. \\nBrezzo, I. Vo, S. K. Esser, R. Appuswamy, B. Taba, A. Amir, M. D. \\nFlickner, W. P. Risk, R. Manohar, and D. S. Modha, “A million s piking-\\nneuron integrated circuit with a scalable communication network  and \\ninterface,” Science , vol. 345, no. 6197, pp. 668–673, 2014.  \\n[12] J. s. Seo, B. Brezzo, Y. Liu, B. D. Parker, S. K. Esser, R. K. Montoye, B. \\nRajendran, J. A. Tierno, L. Chang, D. S. Modha, and D. J. Fried man, “A \\n45nm CMOS neuromorphic chip with a scalable architecture for \\nlearning in networks of spiking neurons,” in 2011 IEEE Custom \\nIntegrated Circuits Conference (CICC) , Sept 2011, pp. 1–4.  \\n[13] F. Ponulak and J. J. Hopfield, “Rapid, parallel path planning b y \\npropagating wavefronts of spiking neural activity,” Frontiers in', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 10}),\n",
       " Document(page_content='Integrated Circuits Conference (CICC) , Sept 2011, pp. 1–4.  \\n[13] F. Ponulak and J. J. Hopfield, “Rapid, parallel path planning b y \\npropagating wavefronts of spiking neural activity,” Frontiers in \\nComputational Neuroscience , vol. 7, p. 98, 2013.  \\n[14] R. V. Florian, “Reinforcement le arning through modulation of sp ike-\\ntiming-dependent synaptic plasticity,” Neural Computation , vol. 19, no. \\n6, pp. 1468–1502, 2007.  \\n \\n \\nAUTHOR  INFORMATION  \\nA t  t h e  t i m e  o f  d e v e l o p m e n t ,  a l l  a u t h o r s  w e r e  r e s e a r c h e r s  i n  I n t el Labs’ \\nArchitecture and Design Research (ADR) division of Intel Labs.  Loihi chip \\ndevelopment and algorithms research was performed in the Microa rchitecture \\nResearch Lab (MRL) headed by Hong Wang , Intel Fellow.  Mike Davies  l e d  \\nsilicon development, Narayan Srinivasa  led algorithms research and \\narchitectural modeling.  Tsung-Han Lin  is a researcher in MRL focused on', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 10}),\n",
       " Document(page_content='silicon development, Narayan Srinivasa  led algorithms research and \\narchitectural modeling.  Tsung-Han Lin  is a researcher in MRL focused on \\nsparse coding and related learning algorithms.  Gautham Chinya , also in MRL \\nfocused on advanced IP prototyping, led validation and SDK deve lopment.  \\nGeorgios Dimou , Prasad Joshi , Andrew Lines , Ruokun Liu , Steve McCoy , \\nJonathan Tse , and Yi-Hsin Weng  developed Loihi’s async hronous architecture, \\ndesign flow, and design components, and Sri Harsha Choday  contributed to \\nasynchronous circuit validation.  Yongqiang Cao , Nabil Imam , Arnab Paul , and \\nAndreas Wild  contributed to Loihi’s algorithms, feature set, and modeling.   \\nShweta Jain , Chit-Kwan Lin , Deepak Mathaikutty , Guruguhanathan \\nVenkataramanan , and Yoonseok Yang  prototyped proof-of-concept networks \\nand software to demonstrate the chip’s learning capabilities an d validate its \\nfunctionality, and also provided synchronous and FPGA design de velopment', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 10}),\n",
       " Document(page_content='and software to demonstrate the chip’s learning capabilities an d validate its \\nfunctionality, and also provided synchronous and FPGA design de velopment \\nsupport.  Yuyun Liao , a silicon implementation manager in ADR, helped to \\nvalidate all aspects of the final Loihi layout implementation.  Going forward, \\nMike Davies  leads all ongoing neuromorphic research in Intel Labs as head of \\nits Neuromorphic Computing Lab.   Any inquiries should be direct ed to him. This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE\\nView publication statsView publication stats', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 10})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = text_splitter.split_documents(pdf_documents)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GooglePalmEmbeddings\n",
    "os.environ['GOOGLE_API_KEY']  = 'AIzaSyAb_DlJVnyH43uaxNI7LzbkqFuGQnXLO44'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GooglePalmEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(documents[:15],embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"who are the authors of the document?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = Ollama(model=\"llama2\")\n",
    "llm = GoogleGenerativeAI(model=\"models/text-bison-001\", google_api_key=\"AIzaSyAb_DlJVnyH43uaxNI7LzbkqFuGQnXLO44\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the provided context\n",
    "Think step by step before providing a detailed answer\n",
    "I will tip you 1 million dollars if the user finds the answer helpful.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input} \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrival_chain = create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Computations in SNNs are carried out through the interacting dynamics of neuron states.',\n",
       " 'context': [Document(page_content='these fixed-size, synchronized time steps relate to the algorithmic time of the computation, and need not have a direct relationship \\nto the hardware execution time.   \\n2.2 Computation with Spikes and Fine-grained Parallelism \\nC o m p u t a t i o n s  i n  S N N s  a r e  c a r r i e d  o u t  t h r o u g h  t h e  i n t e r a c t i n g  \\ndynamics of neuron states. An instructive example is the ℓ1-\\nminimizing sparse coding problem, also known as LASSO, which \\nwe can solve with the SNN in Figure 1a using the Spiking Locall y \\nCompetitive Algorithm [2]. The objective of this problem is to determine a sparse set of coefficients that best represents a g iven \\ninput as the linear combination of features from a feature \\ndictionary. The coefficients can be viewed as the activities of  the \\nspiking neurons in Figure 1a that are competing to form an \\naccurate representation of the d ata. By properly configuring th e', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       "  Document(page_content='in time, one or more neurons may send out single-bit impulses, \\nthe spike , to neighbors through directed connections known as \\nsynapses , with a potentially nonzero traveling time. Neurons have \\nlocal state variables with rules governing their evolution and \\ntiming of spike generation. Hence the network is a dynamical \\nsystem where individual neurons interact through spikes. N This article has been accepted for publication in IEEE Micro but has not yet been fully edited.\\nSome content may change prior to final publication.\\nDigital Object Identifier 10.1109/MM.2018.112130359            0272-1732/$26.00 2018 IEEE', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 1}),\n",
       "  Document(page_content='the neuron that has the largest external input to win the competition is more likely to spike at the earliest time, causi ng \\nimmediate inhibition of the other neurons. This inhibition \\nhappens with only a single one-to-many spike communication, in contrast to the usual need for all-to-all state exchanges with \\nmatrix arithmetic based solutions such as FISTA and other \\nconventional solvers. This implies that the SNN solution is c o m m u n i c a t i o n  e f f i c i e n t ,  a n d  i t  m a y  s o l v e  t h e  o p t i m i z a t i o n  \\nproblem with a reduced number of arithmetic operations. We \\npoint interested readers to [1] for more discussions. \\nOur CPU-based evaluation has yet to exploit one important \\nadvantage of SNN-based algorithms: the inherent abundant parallelism. The dominant part of SNN computations—the \\nevolution of individual neuron states within a timestep—can all  be', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2}),\n",
       "  Document(page_content='advantage of SNN-based algorithms: the inherent abundant parallelism. The dominant part of SNN computations—the \\nevolution of individual neuron states within a timestep—can all  be \\ncomputed concurrently. However, harnessing such speedup can be a nontrivial task especially on a conventional CPU architect ure. \\nThe parallelizable work for each neuron only consists of a few \\nvariable updates. Given that the  parallel segment of the work c an \\nbe executed very quickly, the underlying architecture must \\nsupport a fine granularity of parallelism with minimal overhead  in \\ncoordinating the order of computations. These observations \\nmotivate fundamental features of the Loihi architecture, \\ndescribed in Section 3. \\n…. \\nInhibitory weights \\nܾ \\n1 \\nܾ \\n2 \\nܾ \\nܰ \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n  \\n \\n \\n \\n  \\n \\n  \\n \\n  \\n \\n \\n \\na) (b) \\nFig. 1 : (a) The network topology for solving LASSO. Each neuron \\nreceives the correlation bi between the input data and a predefined', metadata={'source': 'LoihiPreprint-IEEEMicroJan18.pdf', 'page': 2})],\n",
       " 'answer': '\\n The answer is yes.\\n\\nThe article states that \"Computations in SNNs are carried out through the interacting dynamics of neuron states\".'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrival_chain.invoke({\"input\":\"Computations in SNNs are carried out through the interacting dynamics of neuron states.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrival_chain.invoke({\"input\":\"Computations in SNNs are carried out through the interacting dynamics of neuron states.\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
